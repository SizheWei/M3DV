{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Credit: [Rongyao Fang](https://github.com/rongyaofang)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.checkpoint import checkpoint\n",
    "device=torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "from sklearn.metrics import roc_auc_score\n",
    "# from tensorboardX import SummaryWriter\n",
    "import tensorboard_logger as tb_logger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "PARAMS = {\n",
    "    \"input_channels\": 1,\n",
    "    \"output_channels\": 2,\n",
    "    \"input_transform_fn\": lambda x: x / 128. - 1.,\n",
    "    \"input_conv_channels\": 32,\n",
    "    'down_structure': [2,2,2],\n",
    "#     'down_structure': [3, 8, 4],\n",
    "    'activation_fn': lambda: nn.LeakyReLU(0.1, inplace=True),\n",
    "    # \"normalization_fn\": lambda c: nn.GroupNorm(num_groups=4, num_channels=c),\n",
    "    \"normalization_fn\": lambda c: nn.BatchNorm3d(num_features=c),\n",
    "    \"drop_rate\": 0,\n",
    "    'growth_rate': 32,\n",
    "    'bottleneck': 4,\n",
    "    'compression': 2,\n",
    "    'use_memonger': False  # ~2.2x memory efficiency (batch_size), 25%~30% slower on GTX 1080.\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def densenet3d(with_segment=False, snapshot=None, **kwargs):\n",
    "    for k, v in kwargs.items():\n",
    "        assert k in PARAMS\n",
    "        PARAMS[k] = v\n",
    "    print(\"Model hyper-parameters:\", PARAMS)\n",
    "    if with_segment:\n",
    "        model = DenseSharp()\n",
    "        print(\"Using DenseSharp model.\")\n",
    "    else:\n",
    "        model = DenseNet()\n",
    "        print(\"Using DenseNet model.\")\n",
    "    print(model)\n",
    "    if snapshot is None:\n",
    "        initialize(model.modules())\n",
    "        print(\"Random initialized.\")\n",
    "    else:\n",
    "        state_dict = torch.load(snapshot)\n",
    "        model.load_state_dict(state_dict)\n",
    "        print(\"Load weights from `%s`,\" % snapshot)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize(modules):\n",
    "    for m in modules:\n",
    "        if isinstance(m, nn.Conv3d) or isinstance(m, nn.ConvTranspose3d):\n",
    "            nn.init.kaiming_uniform_(m.weight, mode='fan_in')\n",
    "        elif isinstance(m, nn.Linear):\n",
    "            nn.init.kaiming_uniform_(m.weight, mode='fan_in')\n",
    "            m.bias.data.zero_()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvBlock(nn.Sequential):\n",
    "    def __init__(self, in_channels):\n",
    "        super(ConvBlock, self).__init__()\n",
    "\n",
    "        growth_rate = PARAMS['growth_rate']\n",
    "        bottleneck = PARAMS['bottleneck']\n",
    "        activation_fn = PARAMS['activation_fn']\n",
    "        normalization_fn = PARAMS['normalization_fn']\n",
    "\n",
    "        self.in_channels = in_channels\n",
    "        self.growth_rate = growth_rate\n",
    "        self.use_memonger = PARAMS['use_memonger']\n",
    "        self.drop_rate = PARAMS['drop_rate']\n",
    "\n",
    "        # TODO: consider bias term in conv with GN\n",
    "        self.add_module('norm_1', normalization_fn(in_channels))\n",
    "        self.add_module('act_1', activation_fn())\n",
    "        self.add_module('conv_1', nn.Conv3d(in_channels, bottleneck * growth_rate, kernel_size=1, stride=1,\n",
    "                                            padding=0, bias=True))\n",
    "\n",
    "        self.add_module('norm_2', normalization_fn(bottleneck * growth_rate))\n",
    "        self.add_module('act_2', activation_fn())\n",
    "        self.add_module('conv_2', nn.Conv3d(bottleneck * growth_rate, growth_rate, kernel_size=3, stride=1,\n",
    "                                            padding=1, bias=True))\n",
    "\n",
    "    def forward(self, x):\n",
    "        super_forward = super(ConvBlock, self).forward\n",
    "        if self.use_memonger:\n",
    "            new_features = checkpoint(super_forward, x)\n",
    "        else:\n",
    "            new_features = super_forward(x)\n",
    "        if self.drop_rate > 0:\n",
    "            new_features = F.dropout(new_features, p=self.drop_rate, training=self.training)\n",
    "        return torch.cat([x, new_features], 1)\n",
    "\n",
    "    @property\n",
    "    def out_channels(self):\n",
    "        return self.in_channels + self.growth_rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransmitBlock(nn.Sequential):\n",
    "    def __init__(self, in_channels, is_last_block):\n",
    "        super(TransmitBlock, self).__init__()\n",
    "\n",
    "        activation_fn = PARAMS['activation_fn']\n",
    "        normalization_fn = PARAMS['normalization_fn']\n",
    "        compression = PARAMS['compression']\n",
    "\n",
    "        # print(\"in_channels: %s, compression: %s\" % (in_channels, compression))\n",
    "        assert in_channels % compression == 0\n",
    "\n",
    "        self.in_channels = in_channels\n",
    "        self.compression = compression\n",
    "\n",
    "        self.add_module('norm', normalization_fn(in_channels))\n",
    "        self.add_module('act', activation_fn())\n",
    "        if not is_last_block:\n",
    "            self.add_module('conv', nn.Conv3d(in_channels, in_channels // compression, kernel_size=(1, 1, 1),\n",
    "                                              stride=1, padding=0, bias=True))\n",
    "            self.add_module('pool', nn.AvgPool3d(kernel_size=2, stride=2, padding=0))\n",
    "        else:\n",
    "            self.compression = 1\n",
    "\n",
    "    @property\n",
    "    def out_channels(self):\n",
    "        return self.in_channels // self.compression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Lambda(nn.Module):\n",
    "    def __init__(self, lambda_fn):\n",
    "        super(Lambda, self).__init__()\n",
    "        self.lambda_fn = lambda_fn\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.lambda_fn(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DenseNet(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "\n",
    "        super(DenseNet, self).__init__()\n",
    "\n",
    "        input_channels = PARAMS['input_channels']\n",
    "        input_transform_fn = PARAMS['input_transform_fn']\n",
    "        input_conv_channels = PARAMS['input_conv_channels']\n",
    "        normalization_fn = PARAMS['normalization_fn']\n",
    "        activation_fn = PARAMS['activation_fn']\n",
    "        down_structure = PARAMS['down_structure']\n",
    "        output_channels = PARAMS['output_channels']\n",
    "\n",
    "        self.features = nn.Sequential()\n",
    "        if input_transform_fn is not None:\n",
    "            self.features.add_module(\"input_transform\", Lambda(input_transform_fn))\n",
    "        self.features.add_module(\"init_conv\", nn.Conv3d(input_channels, input_conv_channels, kernel_size=3,\n",
    "                                                        stride=1, padding=1, bias=True))\n",
    "        self.features.add_module(\"init_norm\", normalization_fn(input_conv_channels))\n",
    "        self.features.add_module(\"init_act\", activation_fn())\n",
    "\n",
    "        channels = input_conv_channels\n",
    "        for i, num_layers in enumerate(down_structure):\n",
    "            for j in range(num_layers):\n",
    "                conv_layer = ConvBlock(channels)\n",
    "                self.features.add_module('denseblock{}_layer{}'.format(i + 1, j + 1), conv_layer)\n",
    "                channels = conv_layer.out_channels\n",
    "                # print(i, j, channels)\n",
    "\n",
    "            trans_layer = TransmitBlock(channels, is_last_block=i == len(down_structure) - 1)\n",
    "            self.features.add_module('transition%d' % (i + 1), trans_layer)\n",
    "            channels = trans_layer.out_channels\n",
    "\n",
    "        self.classifier = nn.Linear(channels, output_channels)\n",
    "\n",
    "    def forward(self, x, **return_opts):\n",
    "        # o = x\n",
    "        # for i, layer in enumerate(self.features):\n",
    "        #     o = layer(o)\n",
    "        #     print(i, layer, o.shape)\n",
    "\n",
    "        batch_size, _, d, h, w = x.size()\n",
    "\n",
    "        features = self.features(x)\n",
    "        pooled = F.adaptive_avg_pool3d(features, 1).view(batch_size, -1)\n",
    "        scores = self.classifier(pooled)\n",
    "\n",
    "        if len(return_opts) == 0:\n",
    "            return scores\n",
    "\n",
    "        # return other features in one forward\n",
    "        for opt in return_opts:\n",
    "            assert opt in {\"return_features\", \"return_cam\"}\n",
    "        # print(return_opts)\n",
    "\n",
    "        ret = dict(scores=scores)\n",
    "\n",
    "        if 'return_features' in return_opts and return_opts['return_features']:\n",
    "            ret['features'] = features\n",
    "\n",
    "        if 'return_cam' in return_opts and return_opts['return_cam']:\n",
    "            weight = self.classifier.weight.unsqueeze(-1).unsqueeze(-1).unsqueeze(-1)\n",
    "            bias = self.classifier.bias\n",
    "            cam_raw = F.conv3d(features, weight, bias)\n",
    "            cam = F.interpolate(cam_raw, size=(d, h, w), mode='trilinear', align_corners=True)\n",
    "            ret['cam'] = F.softmax(cam, dim=1)\n",
    "            ret['cam_raw'] = F.softmax(cam_raw, dim=1)\n",
    "        return ret"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "import random\n",
    "import torch\n",
    "import os\n",
    "import torch.nn as nn\n",
    "from tqdm import tqdm\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "class ClfDataset(Dataset):\n",
    "\n",
    "    def __init__(self, train=True):\n",
    "        self.train = train\n",
    "        data_dir = './dataset/'\n",
    "        # choose the dataset\n",
    "        patients_train = os.listdir(data_dir+'train_val/')\n",
    "        patients_train.sort(key= lambda x:int(x[9:-4]))\n",
    "        patients_test = os.listdir(data_dir+'test/')\n",
    "        patients_test.sort(key= lambda x:int(x[9:-4]))\n",
    "        labels_df = pd.read_csv('./dataset/info.csv',index_col=0)\n",
    "\n",
    "        self.data_train = []\n",
    "        self.data_test = []\n",
    "        self.labels = []\n",
    "        self.names_train = []\n",
    "        self.names_test = []\n",
    "\n",
    "        for num_train, patient_train in enumerate(patients_train):\n",
    "            patient_train_name = patient_train[0:-4]\n",
    "            self.names_train.append(patient_train_name)\n",
    "            label = labels_df.get_value(patient_train_name, 'lable')\n",
    "            path_train = data_dir + 'train_val/' + patient_train\n",
    "            img_data_train = np.load(path_train)\n",
    "            voxel_train = img_data_train['voxel'].astype(np.int32)\n",
    "#             voxel_train_crop = voxel_train[10:90,10:90,10:90]\n",
    "            voxel_train_crop = voxel_train[20:80,20:80,20:80]\n",
    "#             voxel_train_crop = voxel_train[25:75,25:75,25:75]\n",
    "            self.data_train.append(voxel_train_crop)\n",
    "            self.labels.append(label)\n",
    "\n",
    "        for num_test, patient_test in enumerate(patients_test):\n",
    "            self.names_test.append(patient_test[0:-4])\n",
    "            path_test = data_dir + 'test/' + patient_test\n",
    "            img_data_test = np.load(path_test)\n",
    "            voxel_test = img_data_test['voxel'].astype(np.int32)\n",
    "#             voxel_test_crop = voxel_test[10:90,10:90,10:90]\n",
    "            voxel_test_crop = voxel_test[20:80,20:80,20:80]\n",
    "#             voxel_test_crop = voxel_test[25:75,25:75,25:75]\n",
    "            self.data_test.append(voxel_test_crop)\n",
    "    \n",
    "    def __getitem__(self, item):\n",
    "        if self.train:\n",
    "            patient_data_train = self.data_train[item]\n",
    "            patient_label = self.labels[item]\n",
    "            patient_name_train = self.names_train[item]\n",
    "            return patient_data_train, patient_label, patient_name_train\n",
    "        else:\n",
    "            patient_data_test = self.data_test[item]\n",
    "            patient_name_test = self.names_test[item]\n",
    "            return patient_data_test, patient_name_test\n",
    "        \n",
    "    def __len__(self):\n",
    "        if self.train:\n",
    "            return len(self.labels)\n",
    "        else:\n",
    "            return len(self.data_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model hyper-parameters: {'input_channels': 1, 'output_channels': 2, 'input_transform_fn': <function <lambda> at 0x7fe6acf26620>, 'input_conv_channels': 32, 'down_structure': [2, 2, 2], 'activation_fn': <function <lambda> at 0x7fe6acf266a8>, 'normalization_fn': <function <lambda> at 0x7fe6acf26730>, 'drop_rate': 0, 'growth_rate': 32, 'bottleneck': 4, 'compression': 2, 'use_memonger': True}\n",
      "Using DenseNet model.\n",
      "DenseNet(\n",
      "  (features): Sequential(\n",
      "    (input_transform): Lambda()\n",
      "    (init_conv): Conv3d(1, 32, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))\n",
      "    (init_norm): BatchNorm3d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (init_act): LeakyReLU(negative_slope=0.1, inplace=True)\n",
      "    (denseblock1_layer1): ConvBlock(\n",
      "      (norm_1): BatchNorm3d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (act_1): LeakyReLU(negative_slope=0.1, inplace=True)\n",
      "      (conv_1): Conv3d(32, 128, kernel_size=(1, 1, 1), stride=(1, 1, 1))\n",
      "      (norm_2): BatchNorm3d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (act_2): LeakyReLU(negative_slope=0.1, inplace=True)\n",
      "      (conv_2): Conv3d(128, 32, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))\n",
      "    )\n",
      "    (denseblock1_layer2): ConvBlock(\n",
      "      (norm_1): BatchNorm3d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (act_1): LeakyReLU(negative_slope=0.1, inplace=True)\n",
      "      (conv_1): Conv3d(64, 128, kernel_size=(1, 1, 1), stride=(1, 1, 1))\n",
      "      (norm_2): BatchNorm3d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (act_2): LeakyReLU(negative_slope=0.1, inplace=True)\n",
      "      (conv_2): Conv3d(128, 32, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))\n",
      "    )\n",
      "    (transition1): TransmitBlock(\n",
      "      (norm): BatchNorm3d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (act): LeakyReLU(negative_slope=0.1, inplace=True)\n",
      "      (conv): Conv3d(96, 48, kernel_size=(1, 1, 1), stride=(1, 1, 1))\n",
      "      (pool): AvgPool3d(kernel_size=2, stride=2, padding=0)\n",
      "    )\n",
      "    (denseblock2_layer1): ConvBlock(\n",
      "      (norm_1): BatchNorm3d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (act_1): LeakyReLU(negative_slope=0.1, inplace=True)\n",
      "      (conv_1): Conv3d(48, 128, kernel_size=(1, 1, 1), stride=(1, 1, 1))\n",
      "      (norm_2): BatchNorm3d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (act_2): LeakyReLU(negative_slope=0.1, inplace=True)\n",
      "      (conv_2): Conv3d(128, 32, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))\n",
      "    )\n",
      "    (denseblock2_layer2): ConvBlock(\n",
      "      (norm_1): BatchNorm3d(80, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (act_1): LeakyReLU(negative_slope=0.1, inplace=True)\n",
      "      (conv_1): Conv3d(80, 128, kernel_size=(1, 1, 1), stride=(1, 1, 1))\n",
      "      (norm_2): BatchNorm3d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (act_2): LeakyReLU(negative_slope=0.1, inplace=True)\n",
      "      (conv_2): Conv3d(128, 32, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))\n",
      "    )\n",
      "    (transition2): TransmitBlock(\n",
      "      (norm): BatchNorm3d(112, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (act): LeakyReLU(negative_slope=0.1, inplace=True)\n",
      "      (conv): Conv3d(112, 56, kernel_size=(1, 1, 1), stride=(1, 1, 1))\n",
      "      (pool): AvgPool3d(kernel_size=2, stride=2, padding=0)\n",
      "    )\n",
      "    (denseblock3_layer1): ConvBlock(\n",
      "      (norm_1): BatchNorm3d(56, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (act_1): LeakyReLU(negative_slope=0.1, inplace=True)\n",
      "      (conv_1): Conv3d(56, 128, kernel_size=(1, 1, 1), stride=(1, 1, 1))\n",
      "      (norm_2): BatchNorm3d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (act_2): LeakyReLU(negative_slope=0.1, inplace=True)\n",
      "      (conv_2): Conv3d(128, 32, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))\n",
      "    )\n",
      "    (denseblock3_layer2): ConvBlock(\n",
      "      (norm_1): BatchNorm3d(88, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (act_1): LeakyReLU(negative_slope=0.1, inplace=True)\n",
      "      (conv_1): Conv3d(88, 128, kernel_size=(1, 1, 1), stride=(1, 1, 1))\n",
      "      (norm_2): BatchNorm3d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (act_2): LeakyReLU(negative_slope=0.1, inplace=True)\n",
      "      (conv_2): Conv3d(128, 32, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))\n",
      "    )\n",
      "    (transition3): TransmitBlock(\n",
      "      (norm): BatchNorm3d(120, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (act): LeakyReLU(negative_slope=0.1, inplace=True)\n",
      "    )\n",
      "  )\n",
      "  (classifier): Linear(in_features=120, out_features=2, bias=True)\n",
      ")\n",
      "Random initialized.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/vivi/anaconda3/envs/3dunet/lib/python3.7/site-packages/ipykernel_launcher.py:33: FutureWarning: get_value is deprecated and will be removed in a future release. Please use .at[] or .iat[] accessors instead\n"
     ]
    }
   ],
   "source": [
    "# '''[begin] load the train and validation dataset:'''\n",
    "# import numpy as np\n",
    "# # use np.load to import data, devide dataset into 2 parts: train_data & validation_data:\n",
    "# # credit: cheez & Matthew Kerian\n",
    "# # link: https://stackoverflow.com/questions/55890813/how-to-fix-object-arrays-cannot-be-loaded-when-allow-pickle-false-for-imdb-loa/56243777\n",
    "# '''use older to successfully load the data:'''\n",
    "# # save np.load\n",
    "# np_load_old = np.load\n",
    "\n",
    "# # modify the default parameters of np.load\n",
    "# np.load = lambda *a,**k: np_load_old(*a, allow_pickle=True, **k)\n",
    "\n",
    "# data_ineed = np.load('muchdata.npy')\n",
    "\n",
    "# # restore np.load for future normal usage\n",
    "# np.load = np_load_old\n",
    "\n",
    "# # If you are working with the basic sample data, use maybe 2 instead of 100 here... you don't have enough data to really do this\n",
    "# train_data = data_ineed[:-60]\n",
    "# validation_data = data_ineed[-60:]\n",
    "# '''[end] load the train and validation dataset:'''\n",
    "\n",
    "model = densenet3d(with_segment=False, use_memonger=True).cuda()\n",
    "# model = nn.DataParallel(model, device_ids=[0, 1])\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "# optimizer = optim.SGD(model.parameters(),lr=0.001, momentum=0.9)\n",
    "\n",
    "criterion = criterion.cuda()\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "data_train = ClfDataset(train=True)\n",
    "train_data_loader = DataLoader(dataset=data_train, batch_size=32, shuffle=True)\n",
    "# dev_data_loader = DataLoader(dataset=data_train, batch_size=32, shuffle=False)\n",
    "data_test = ClfDataset(train=False)\n",
    "test_data_loader = DataLoader(dataset=data_test, batch_size=32, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'candidate11'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_test[0][1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train & Valiation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 15/15 [00:08<00:00,  1.82it/s]\n",
      "  0%|          | 0/15 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1] loss: 0.03059\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 15/15 [00:08<00:00,  1.83it/s]\n",
      "  0%|          | 0/15 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2] loss: 0.02702\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 15/15 [00:07<00:00,  1.91it/s]\n",
      "  0%|          | 0/15 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[3] loss: 0.02708\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 15/15 [00:08<00:00,  1.87it/s]\n",
      "  0%|          | 0/15 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[4] loss: 0.02612\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 15/15 [00:08<00:00,  1.85it/s]\n",
      "  0%|          | 0/15 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[5] loss: 0.02555\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 15/15 [00:08<00:00,  1.81it/s]\n",
      "  0%|          | 0/15 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[6] loss: 0.02483\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 15/15 [00:08<00:00,  1.79it/s]\n",
      "  0%|          | 0/15 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[7] loss: 0.02439\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 15/15 [00:08<00:00,  1.79it/s]\n",
      "  0%|          | 0/15 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[8] loss: 0.02515\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 15/15 [00:08<00:00,  1.80it/s]\n",
      "  0%|          | 0/15 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[9] loss: 0.02498\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 15/15 [00:08<00:00,  1.73it/s]\n",
      "  0%|          | 0/15 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[10] loss: 0.02475\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 15/15 [00:08<00:00,  1.68it/s]\n",
      "  0%|          | 0/15 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[11] loss: 0.02370\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 15/15 [00:09<00:00,  1.66it/s]\n",
      "  0%|          | 0/15 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[12] loss: 0.02346\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 15/15 [00:08<00:00,  1.80it/s]\n",
      "  0%|          | 0/15 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[13] loss: 0.02259\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 15/15 [00:08<00:00,  1.73it/s]\n",
      "  0%|          | 0/15 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[14] loss: 0.02370\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 15/15 [00:08<00:00,  1.68it/s]\n",
      "  0%|          | 0/15 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[15] loss: 0.02354\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 15/15 [00:08<00:00,  1.73it/s]\n",
      "  0%|          | 0/15 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[16] loss: 0.02328\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 15/15 [00:08<00:00,  1.68it/s]\n",
      "  0%|          | 0/15 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[17] loss: 0.02290\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 15/15 [00:08<00:00,  1.73it/s]\n",
      "  0%|          | 0/15 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[18] loss: 0.02131\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 15/15 [00:08<00:00,  1.81it/s]\n",
      "  0%|          | 0/15 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[19] loss: 0.02091\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 15/15 [00:09<00:00,  1.67it/s]\n",
      "  0%|          | 0/15 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[20] loss: 0.02086\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 15/15 [00:08<00:00,  1.68it/s]\n",
      "  0%|          | 0/15 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[21] loss: 0.02102\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 15/15 [00:08<00:00,  1.68it/s]\n",
      "  0%|          | 0/15 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[22] loss: 0.02047\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 15/15 [00:08<00:00,  1.77it/s]\n",
      "  0%|          | 0/15 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[23] loss: 0.02016\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 15/15 [00:08<00:00,  1.68it/s]\n",
      "  0%|          | 0/15 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[24] loss: 0.02063\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 15/15 [00:08<00:00,  1.67it/s]\n",
      "  0%|          | 0/15 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[25] loss: 0.01955\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 15/15 [00:08<00:00,  1.69it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[26] loss: 0.01845\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "date = 25 # day\n",
    "number = 14 # No.\n",
    "sample_size = 32\n",
    "file_name = 'train_{}_{}'.format(date,number)\n",
    "# writer = SummaryWriter('./runs/{}'.format(file_name))\n",
    "logger = tb_logger.Logger(logdir = './runs/train_{}_{}'.format(date,number), flush_secs=2)\n",
    "# val_logger = tb_logger.Logger(logdir = './runs/train_{}_{}/test'.format(date,number), flush_secs=2)\n",
    "\n",
    "names_box_train = list()\n",
    "labels_box_train = []\n",
    "\n",
    "for epoch in range(26):  # loop over the dataset multiple times\n",
    "    \n",
    "    # for train:\n",
    "    running_loss = 0.0\n",
    "    # for valiation:\n",
    "    dev_loss = 0\n",
    "    predict_value = torch.zeros(1) # valiation predict:\n",
    "    true_value = torch.zeros(1) # true lable of valiation part:\n",
    "    \n",
    "    for index, (inputs, labels, patient_name) in enumerate(tqdm(train_data_loader)):\n",
    "        \n",
    "        names_box_train.extend(patient_name)\n",
    "        labels_box_train.extend(labels.cpu().numpy().tolist())\n",
    "        \n",
    "        if index<=110:# train part: \n",
    "            \n",
    "            model.train()\n",
    "            \n",
    "            # zero the parameter gradients\n",
    "            optimizer.zero_grad()\n",
    "        \n",
    "            inputs, labels = inputs.cuda(), labels.cuda()\n",
    "        \n",
    "            # forward + backward + optimize\n",
    "            inputs = inputs.unsqueeze(dim=1).float()\n",
    "        \n",
    "            inputs = F.interpolate(inputs, size=[sample_size,sample_size,sample_size],mode='trilinear',align_corners=False)\n",
    "        \n",
    "            outputs = model(inputs)\n",
    "        \n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        \n",
    "            # print statistics\n",
    "            running_loss += loss.item()\n",
    "            \n",
    "        else: # valiation part:\n",
    "            \n",
    "            model.eval()\n",
    "            \n",
    "            inputs, labels = inputs.cuda(), labels.cuda()\n",
    "        \n",
    "            # forward + backward + optimize\n",
    "            inputs = inputs.unsqueeze(dim=1).float()\n",
    "        \n",
    "            inputs = F.interpolate(inputs, size=[sample_size,sample_size,sample_size],mode='trilinear',align_corners=False)\n",
    "        \n",
    "            outputs = model(inputs)\n",
    "        \n",
    "            test_value = F.softmax(outputs)\n",
    "            test_value_1 = test_value[:,1]\n",
    "            test_value_1 = test_value_1.cpu()\n",
    "            predict_value = torch.cat((predict_value,test_value_1))\n",
    "    \n",
    "            test_value_2 = labels\n",
    "            test_value_2 = test_value_2.cpu().float()\n",
    "            true_value = torch.cat((true_value, test_value_2))\n",
    "            \n",
    "    print('[%d] loss: %.5f' %(epoch + 1, running_loss / 384))\n",
    "#     writer.add_scalar('Loss', running_loss, epoch)\n",
    "    logger.log_value('loss',running_loss,epoch)\n",
    "    running_loss = 0.0\n",
    "        \n",
    "#     predict_value = predict_value.detach().numpy()\n",
    "#     true_value = true_value.numpy()\n",
    "#     auc_score = roc_auc_score(true_value, predict_value)\n",
    "#     print(auc_score)\n",
    "# #     writer.add_scalar('Auc', auc_score, epoch)\n",
    "#     logger.log_value('AUC',auc_score, epoch)\n",
    "    \n",
    "# writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import csv\n",
    "# box_train = zip(names_box_train, labels_box_train)\n",
    "# # print(box_train)\n",
    "# with open('output.csv','w') as result_file:\n",
    "#     wr = csv.writer(result_file, dialect='excel')\n",
    "#     for row in box_train:\n",
    "#         wr.writerow(row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH = './my_{}_{}.pth'.format(date,number)\n",
    "torch.save(model.state_dict(), PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ./my_23_1.pth 100epochs\n",
    "# my_24_1.pth 300epochs train&dev Adam 0.62(200) batch_size = 32\n",
    "# my_24_2.pth 200epochs train&dev SGD 0.656(200) batch_size = 32\n",
    "# my_24_3.pth 200epochs train&dev SGD 0.633(200) batch_size = 16\n",
    "# my_24_4.pth 200epochs train&dev SGD 0.625(200) batch_size = 32 crop 80-> 32\n",
    "# my_24_5.pth 200epochs train&dev Adam 0.600(200) batch_size = 32 crop 80-> 32 0.689(20)\n",
    "# my_24_6.pth  25epochs train&dev Adam           batch_size = 32 crop 80-> 32  0.689(25)\n",
    "# my_24_7.pth  25epochs train     Adam           batch_size = 32 crop 80-> 32             test:  0.5\n",
    "# my_24_8.pth 200epochs train&dev Adam 0.54(200) batch_size = 64 crop 80-> 32             [3，8，4] notes: 不收敛 下一步调小batch，增加采样点数\n",
    "# my_25_1.pth 200epochs train&dev Adam 0.59(200) batch_size = 32 crop 80-> 32  0.689(27) [3，8，4] notes: 收敛 下一步调小batch，增加采样点数\n",
    "# my_25_2.pth 200epochs train&dev Adam 0.64(80) batch_size = 32 crop 60-> 32  0.76(22) [2，2，2] \n",
    "# my_25_3.pth 200epochs train&dev Adam 0.58(70) batch_size = 32 crop 50-> 32  0.65(39) [2，2，2] notes: 收敛 下一步恢复原来的crop 然后调整网络结构和优化器\n",
    "# my_25_4.pth 150epochs train&dev Adam 0.62(63) batch_size = 32 crop 60-> 32  0.64(22) [2,2,2] notes: \n",
    "# my_25_5.pth 150epochs train&dev Adam 0.63(150) batch_size = 32 crop 60-> 32 0.64(44) [3,8,4] notes: \n",
    "# my_25_6.pth 150epochs train&dev Adam 0.63(60) batch_size = 32 crop 60-> 32 0.64(58) [3,8,4] notes: \n",
    "# my_25_7.pth 150epochs train&dev SGD 0.64(88) batch_size = 32 crop 60-> 32 0.68(20) [3,8,4] notes: \n",
    "# my_25_8.pth 24epochs train SGD \\        batch_size = 32 crop 60-> 32 \\        [3,8,4] test: 0.56\n",
    "# my_25_9.pth 22epochs train SGD \\        batch_size = 32 crop 60-> 32 \\        [3,8,4] test: 0.52\n",
    "# my_25_10.pth 30epochs train SGD \\        batch_size = 32 crop 60-> 32 \\        [3,8,4] test: 0.\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/4 [00:00<?, ?it/s]/home/vivi/anaconda3/envs/3dunet/lib/python3.7/site-packages/ipykernel_launcher.py:24: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "100%|██████████| 4/4 [00:00<00:00,  6.12it/s]\n"
     ]
    }
   ],
   "source": [
    "# date = 24 # day\n",
    "# number = 7 # No.\n",
    "# PATH = './my_{}_{}.pth'.format(date,number)\n",
    "# model = densenet3d(with_segment=False, use_memonger=True).cuda()\n",
    "# model.load_state_dict(torch.load(PATH))\n",
    "\n",
    "model.eval()\n",
    "# dev_loss = 0\n",
    "predict_value = []\n",
    "# predict_value = np.array(predict_value)\n",
    "# predict_value = torch.from_numpy(predict_value)\n",
    "names_box_test = []\n",
    "\n",
    "for index, (inputs,patient_name) in enumerate(tqdm(test_data_loader)):\n",
    "    \n",
    "    inputs = inputs.cuda()\n",
    "        # forward + backward + optimize\n",
    "    inputs = inputs.unsqueeze(dim=1).float()\n",
    "        \n",
    "    inputs = F.interpolate(inputs, size=[32,32,32],mode='trilinear',align_corners=False)\n",
    "        \n",
    "    outputs = model(inputs)\n",
    "    \n",
    "    test_value = F.softmax(outputs)\n",
    "    test_value_1 = test_value[:,1]\n",
    "    test_value_1 = test_value_1.cpu()\n",
    "    predict_value.extend(test_value_1.detach().numpy().tolist())\n",
    "#     print(predict_value)\n",
    "    \n",
    "#     predict_value = torch.cat((predict_value,test_value_1))\n",
    "    names_box_test.extend(patient_name)\n",
    "    \n",
    "#     if index==1:\n",
    "#         break\n",
    "    \n",
    "#     test_value_2 = labels\n",
    "#     test_value_2 = test_value_2.cpu().float()\n",
    "#     true_value = torch.cat((true_value, test_value_2))\n",
    "\n",
    "\n",
    "# test_predict_value = predict_value.detach().numpy()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------Success, please check output_25_14.csv---------\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "box_train = zip(names_box_test, predict_value)\n",
    "# print(box_train)\n",
    "with open('output_{}_{}.csv'.format(date,number),'w') as result_file:\n",
    "    wr = csv.writer(result_file, dialect='excel')\n",
    "    for row in box_train:\n",
    "        wr.writerow(row)\n",
    "        \n",
    "print('---------Success, please check output_{}_{}.csv---------'.format(date,number))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for epoch in range(4):  # loop over the dataset multiple times\n",
    "#     running_loss = 0.0\n",
    "#     for inputs, labels in train_data_loader:\n",
    "#         # get the inputs; data is a list of [inputs, labels]\n",
    "#         # inputs: tensor(100*(100*100)), label\n",
    "#         inputs_numpy, labels = data\n",
    "#         inputs = torch.from_numpy(inputs_numpy)\n",
    "#         inputs = inputs.unsqueeze(0)\n",
    "#         inputs = inputs.unsqueeze(0)\n",
    "#         inputs = inputs.cuda()\n",
    "# #         inputs = inputs.float()\n",
    "#         labels = torch.from_numpy(np.asarray(labels)).float()\n",
    "#         labels = labels.long() #credit: ptrblck; link: https://discuss.pytorch.org/t/runtimeerror-expected-object-of-scalar-type-long-but-got-scalar-type-float-when-using-crossentropyloss/30542/4\n",
    "#         labels = labels.unsqueeze(0)\n",
    "\n",
    "#         labels = labels.cuda()\n",
    "#         # zero the parameter gradients\n",
    "#         optimizer.zero_grad()\n",
    "\n",
    "#         # forward + backward + optimize\n",
    "#         outputs = model(inputs)\n",
    "        \n",
    "#         loss = criterion(outputs, labels)\n",
    "#         loss.backward()\n",
    "#         optimizer.step()\n",
    "\n",
    "#         # print statistics\n",
    "#         running_loss += loss.item()\n",
    "#         if i % 20 == 19:    # print every 2000 mini-batches\n",
    "#             print('[%d, %5d] loss: %.3f' %\n",
    "#                   (epoch + 1, i + 1, running_loss / 20))\n",
    "#             running_loss = 0.0\n",
    "\n",
    "# print('Finished Training')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Only train:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# for epoch in range(100):  # loop over the dataset multiple times\n",
    "    \n",
    "#     model.train()\n",
    "#     running_loss = 0.0\n",
    "#     for index, (inputs, labels) in enumerate(tqdm(train_data_loader)):\n",
    "#         # zero the parameter gradients\n",
    "#         optimizer.zero_grad()\n",
    "        \n",
    "#         inputs, labels = inputs.cuda(), labels.cuda()\n",
    "        \n",
    "#         # forward + backward + optimize\n",
    "#         inputs = inputs.unsqueeze(dim=1).float()\n",
    "        \n",
    "#         inputs = F.interpolate(inputs, size=[32,32,32],mode='trilinear',align_corners=False)\n",
    "        \n",
    "#         outputs = model(inputs)\n",
    "        \n",
    "#         loss = criterion(outputs, labels)\n",
    "#         loss.backward()\n",
    "#         optimizer.step()\n",
    "        \n",
    "#         # print statistics\n",
    "#         running_loss += loss.item()\n",
    "        \n",
    "#         if index==11:\n",
    "#             break\n",
    "        \n",
    "\n",
    "#     print('[%d] loss: %.5f' %\n",
    "#         (epoch + 1, running_loss / 352))\n",
    "#     running_loss = 0.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Only Valiation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PATH = './my_23_1.pth'\n",
    "# model = densenet3d(with_segment=False, use_memonger=True).cuda()\n",
    "# model.load_state_dict(torch.load(PATH))\n",
    "\n",
    "# model.eval()\n",
    "# dev_loss = 0\n",
    "# predict_value = torch.zeros(1)\n",
    "# true_value = torch.zeros(1)\n",
    "\n",
    "# for index, (inputs, labels) in enumerate(tqdm(train_data_loader)):\n",
    "#     if index<=11:\n",
    "#         continue\n",
    "\n",
    "#     inputs, labels = inputs.cuda(), labels.cuda()\n",
    "        \n",
    "#         # forward + backward + optimize\n",
    "#     inputs = inputs.unsqueeze(dim=1).float()\n",
    "        \n",
    "#     inputs = F.interpolate(inputs, size=[32,32,32],mode='trilinear',align_corners=False)\n",
    "        \n",
    "#     outputs = model(inputs)\n",
    "    \n",
    "#     test_value = F.softmax(outputs)\n",
    "#     test_value_1 = test_value[:,1]\n",
    "#     test_value_1 = test_value_1.cpu()\n",
    "#     predict_value = torch.cat((predict_value,test_value_1))\n",
    "    \n",
    "#     test_value_2 = labels\n",
    "#     test_value_2 = test_value_2.cpu().float()\n",
    "#     true_value = torch.cat((true_value, test_value_2))\n",
    "# #     loss = criterion(outputs, labels)    \n",
    "# #     dev_loss+=loss.item()\n",
    "    \n",
    "# # dev_loss = dev_loss/64\n",
    "# # print(\"%.4f\" %dev_loss)\n",
    "\n",
    "# predict_value = predict_value.detach().numpy()\n",
    "# true_value = true_value.numpy()\n",
    "\n",
    "# from sklearn.metrics import roc_auc_score\n",
    "# roc_auc_score(true_value, predict_value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 验证加入名字之后是否对应"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test Part"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # date = 24 # day\n",
    "# # number = 7 # No.\n",
    "# # PATH = './my_{}_{}.pth'.format(date,number)\n",
    "# # model = densenet3d(with_segment=False, use_memonger=True).cuda()\n",
    "# # model.load_state_dict(torch.load(PATH))\n",
    "\n",
    "# model.eval()\n",
    "# # dev_loss = 0\n",
    "# predict_value = torch.zeros(1)\n",
    "# # true_value = torch.zeros(1)\n",
    "\n",
    "# for _, inputs in enumerate(tqdm(test_data_loader)):\n",
    "    \n",
    "#     inputs = inputs.cuda()\n",
    "#         # forward + backward + optimize\n",
    "#     inputs = inputs.unsqueeze(dim=1).float()\n",
    "        \n",
    "#     inputs = F.interpolate(inputs, size=[32,32,32],mode='trilinear',align_corners=False)\n",
    "        \n",
    "#     outputs = model(inputs)\n",
    "    \n",
    "#     test_value = F.softmax(outputs)\n",
    "#     test_value_1 = test_value[:,1]\n",
    "#     test_value_1 = test_value_1.cpu()\n",
    "#     print(test_value_1)\n",
    "    \n",
    "#     predict_value = torch.cat((predict_value,test_value_1))\n",
    "    \n",
    "# #     test_value_2 = labels\n",
    "# #     test_value_2 = test_value_2.cpu().float()\n",
    "# #     true_value = torch.cat((true_value, test_value_2))\n",
    "\n",
    "\n",
    "# test_predict_value = predict_value.detach().numpy()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# np.savetxt('new_{}_{}.csv'.format(date,number), test_predict_value, delimiter = ',')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
