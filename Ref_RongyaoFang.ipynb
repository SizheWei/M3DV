{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Credit: [Rongyao Fang](https://github.com/rongyaofang)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.checkpoint import checkpoint\n",
    "device=torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "PARAMS = {\n",
    "    \"input_channels\": 1,\n",
    "    \"output_channels\": 2,\n",
    "    \"input_transform_fn\": lambda x: x / 128. - 1.,\n",
    "    \"input_conv_channels\": 32,\n",
    "    'down_structure': [2,2,2],\n",
    "#     'down_structure': [3, 8, 4],\n",
    "    'activation_fn': lambda: nn.LeakyReLU(0.1, inplace=True),\n",
    "    # \"normalization_fn\": lambda c: nn.GroupNorm(num_groups=4, num_channels=c),\n",
    "    \"normalization_fn\": lambda c: nn.BatchNorm3d(num_features=c),\n",
    "    \"drop_rate\": 0,\n",
    "    'growth_rate': 32,\n",
    "    'bottleneck': 4,\n",
    "    'compression': 2,\n",
    "    'use_memonger': True  # ~2.2x memory efficiency (batch_size), 25%~30% slower on GTX 1080.\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def densenet3d(with_segment=False, snapshot=None, **kwargs):\n",
    "    for k, v in kwargs.items():\n",
    "        assert k in PARAMS\n",
    "        PARAMS[k] = v\n",
    "    print(\"Model hyper-parameters:\", PARAMS)\n",
    "    if with_segment:\n",
    "        model = DenseSharp()\n",
    "        print(\"Using DenseSharp model.\")\n",
    "    else:\n",
    "        model = DenseNet()\n",
    "        print(\"Using DenseNet model.\")\n",
    "    print(model)\n",
    "    if snapshot is None:\n",
    "        initialize(model.modules())\n",
    "        print(\"Random initialized.\")\n",
    "    else:\n",
    "        state_dict = torch.load(snapshot)\n",
    "        model.load_state_dict(state_dict)\n",
    "        print(\"Load weights from `%s`,\" % snapshot)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize(modules):\n",
    "    for m in modules:\n",
    "        if isinstance(m, nn.Conv3d) or isinstance(m, nn.ConvTranspose3d):\n",
    "            nn.init.kaiming_uniform_(m.weight, mode='fan_in')\n",
    "        elif isinstance(m, nn.Linear):\n",
    "            nn.init.kaiming_uniform_(m.weight, mode='fan_in')\n",
    "            m.bias.data.zero_()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvBlock(nn.Sequential):\n",
    "    def __init__(self, in_channels):\n",
    "        super(ConvBlock, self).__init__()\n",
    "\n",
    "        growth_rate = PARAMS['growth_rate']\n",
    "        bottleneck = PARAMS['bottleneck']\n",
    "        activation_fn = PARAMS['activation_fn']\n",
    "        normalization_fn = PARAMS['normalization_fn']\n",
    "\n",
    "        self.in_channels = in_channels\n",
    "        self.growth_rate = growth_rate\n",
    "        self.use_memonger = PARAMS['use_memonger']\n",
    "        self.drop_rate = PARAMS['drop_rate']\n",
    "\n",
    "        # TODO: consider bias term in conv with GN\n",
    "        self.add_module('norm_1', normalization_fn(in_channels))\n",
    "        self.add_module('act_1', activation_fn())\n",
    "        self.add_module('conv_1', nn.Conv3d(in_channels, bottleneck * growth_rate, kernel_size=1, stride=1,\n",
    "                                            padding=0, bias=True))\n",
    "\n",
    "        self.add_module('norm_2', normalization_fn(bottleneck * growth_rate))\n",
    "        self.add_module('act_2', activation_fn())\n",
    "        self.add_module('conv_2', nn.Conv3d(bottleneck * growth_rate, growth_rate, kernel_size=3, stride=1,\n",
    "                                            padding=1, bias=True))\n",
    "\n",
    "    def forward(self, x):\n",
    "        super_forward = super(ConvBlock, self).forward\n",
    "        if self.use_memonger:\n",
    "            new_features = checkpoint(super_forward, x)\n",
    "        else:\n",
    "            new_features = super_forward(x)\n",
    "        if self.drop_rate > 0:\n",
    "            new_features = F.dropout(new_features, p=self.drop_rate, training=self.training)\n",
    "        return torch.cat([x, new_features], 1)\n",
    "\n",
    "    @property\n",
    "    def out_channels(self):\n",
    "        return self.in_channels + self.growth_rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransmitBlock(nn.Sequential):\n",
    "    def __init__(self, in_channels, is_last_block):\n",
    "        super(TransmitBlock, self).__init__()\n",
    "\n",
    "        activation_fn = PARAMS['activation_fn']\n",
    "        normalization_fn = PARAMS['normalization_fn']\n",
    "        compression = PARAMS['compression']\n",
    "\n",
    "        # print(\"in_channels: %s, compression: %s\" % (in_channels, compression))\n",
    "        assert in_channels % compression == 0\n",
    "\n",
    "        self.in_channels = in_channels\n",
    "        self.compression = compression\n",
    "\n",
    "        self.add_module('norm', normalization_fn(in_channels))\n",
    "        self.add_module('act', activation_fn())\n",
    "        if not is_last_block:\n",
    "            self.add_module('conv', nn.Conv3d(in_channels, in_channels // compression, kernel_size=(1, 1, 1),\n",
    "                                              stride=1, padding=0, bias=True))\n",
    "            self.add_module('pool', nn.AvgPool3d(kernel_size=2, stride=2, padding=0))\n",
    "        else:\n",
    "            self.compression = 1\n",
    "\n",
    "    @property\n",
    "    def out_channels(self):\n",
    "        return self.in_channels // self.compression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Lambda(nn.Module):\n",
    "    def __init__(self, lambda_fn):\n",
    "        super(Lambda, self).__init__()\n",
    "        self.lambda_fn = lambda_fn\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.lambda_fn(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DenseNet(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "\n",
    "        super(DenseNet, self).__init__()\n",
    "\n",
    "        input_channels = PARAMS['input_channels']\n",
    "        input_transform_fn = PARAMS['input_transform_fn']\n",
    "        input_conv_channels = PARAMS['input_conv_channels']\n",
    "        normalization_fn = PARAMS['normalization_fn']\n",
    "        activation_fn = PARAMS['activation_fn']\n",
    "        down_structure = PARAMS['down_structure']\n",
    "        output_channels = PARAMS['output_channels']\n",
    "\n",
    "        self.features = nn.Sequential()\n",
    "        if input_transform_fn is not None:\n",
    "            self.features.add_module(\"input_transform\", Lambda(input_transform_fn))\n",
    "        self.features.add_module(\"init_conv\", nn.Conv3d(input_channels, input_conv_channels, kernel_size=3,\n",
    "                                                        stride=1, padding=1, bias=True))\n",
    "        self.features.add_module(\"init_norm\", normalization_fn(input_conv_channels))\n",
    "        self.features.add_module(\"init_act\", activation_fn())\n",
    "\n",
    "        channels = input_conv_channels\n",
    "        for i, num_layers in enumerate(down_structure):\n",
    "            for j in range(num_layers):\n",
    "                conv_layer = ConvBlock(channels)\n",
    "                self.features.add_module('denseblock{}_layer{}'.format(i + 1, j + 1), conv_layer)\n",
    "                channels = conv_layer.out_channels\n",
    "                # print(i, j, channels)\n",
    "\n",
    "            trans_layer = TransmitBlock(channels, is_last_block=i == len(down_structure) - 1)\n",
    "            self.features.add_module('transition%d' % (i + 1), trans_layer)\n",
    "            channels = trans_layer.out_channels\n",
    "\n",
    "        self.classifier = nn.Linear(channels, output_channels)\n",
    "\n",
    "    def forward(self, x, **return_opts):\n",
    "        # o = x\n",
    "        # for i, layer in enumerate(self.features):\n",
    "        #     o = layer(o)\n",
    "        #     print(i, layer, o.shape)\n",
    "\n",
    "        batch_size, _, d, h, w = x.size()\n",
    "\n",
    "        features = self.features(x)\n",
    "        pooled = F.adaptive_avg_pool3d(features, 1).view(batch_size, -1)\n",
    "        scores = self.classifier(pooled)\n",
    "\n",
    "        if len(return_opts) == 0:\n",
    "            return scores\n",
    "\n",
    "        # return other features in one forward\n",
    "        for opt in return_opts:\n",
    "            assert opt in {\"return_features\", \"return_cam\"}\n",
    "        # print(return_opts)\n",
    "\n",
    "        ret = dict(scores=scores)\n",
    "\n",
    "        if 'return_features' in return_opts and return_opts['return_features']:\n",
    "            ret['features'] = features\n",
    "\n",
    "        if 'return_cam' in return_opts and return_opts['return_cam']:\n",
    "            weight = self.classifier.weight.unsqueeze(-1).unsqueeze(-1).unsqueeze(-1)\n",
    "            bias = self.classifier.bias\n",
    "            cam_raw = F.conv3d(features, weight, bias)\n",
    "            cam = F.interpolate(cam_raw, size=(d, h, w), mode='trilinear', align_corners=True)\n",
    "            ret['cam'] = F.softmax(cam, dim=1)\n",
    "            ret['cam_raw'] = F.softmax(cam_raw, dim=1)\n",
    "        return ret"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "import random\n",
    "import torch\n",
    "import os\n",
    "import torch.nn as nn\n",
    "from tqdm import tqdm\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "class ClfDataset(Dataset):\n",
    "\n",
    "    def __init__(self, train=True):\n",
    "        self.train = train\n",
    "        data_dir = './dataset/'\n",
    "        # choose the dataset\n",
    "        patients_train = os.listdir(data_dir+'train_val/')\n",
    "        patients_train.sort()\n",
    "        patients_test = os.listdir(data_dir+'test/')\n",
    "        patients_test.sort()\n",
    "        labels_df = pd.read_csv('./dataset/info.csv',index_col=0)\n",
    "\n",
    "        self.data_train = []\n",
    "        self.data_test = []\n",
    "        self.label = []\n",
    "\n",
    "        for num, patient in enumerate(patients_train):\n",
    "            patient_name = patient[0:-4]\n",
    "            label = labels_df.get_value(patient_name, 'lable')\n",
    "            path = data_dir + 'train_val/' + patient\n",
    "            img_data = np.load(path)\n",
    "            voxel = img_data['voxel'].astype(np.int32)\n",
    "            self.data_train.append(voxel)\n",
    "            self.label.append(label)\n",
    "\n",
    "        for num, patient in enumerate(patients_test):\n",
    "            path = data_dir + 'test/' + patient\n",
    "            img_data = np.load(path)\n",
    "            voxel = img_data['voxel'].astype(np.int32)\n",
    "            self.data_test.append(voxel)\n",
    "    \n",
    "    def __getitem__(self, item):\n",
    "        if self.train:\n",
    "            patient_data = self.data_train[item]\n",
    "            patient_label = self.label[item]\n",
    "            return patient_data, patient_label\n",
    "        else:\n",
    "            patient_data = self.data_test[item]\n",
    "            return patient_data\n",
    "        \n",
    "    def __len__(self):\n",
    "        if self.train:\n",
    "            return len(self.label)\n",
    "        else:\n",
    "            return len(self.data_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model hyper-parameters: {'input_channels': 1, 'output_channels': 2, 'input_transform_fn': <function <lambda> at 0x7f50b97c6ae8>, 'input_conv_channels': 32, 'down_structure': [2, 2, 2], 'activation_fn': <function <lambda> at 0x7f50b97c6b70>, 'normalization_fn': <function <lambda> at 0x7f50b97c6bf8>, 'drop_rate': 0, 'growth_rate': 32, 'bottleneck': 4, 'compression': 2, 'use_memonger': True}\n",
      "Using DenseNet model.\n",
      "DenseNet(\n",
      "  (features): Sequential(\n",
      "    (input_transform): Lambda()\n",
      "    (init_conv): Conv3d(1, 32, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))\n",
      "    (init_norm): BatchNorm3d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (init_act): LeakyReLU(negative_slope=0.1, inplace=True)\n",
      "    (denseblock1_layer1): ConvBlock(\n",
      "      (norm_1): BatchNorm3d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (act_1): LeakyReLU(negative_slope=0.1, inplace=True)\n",
      "      (conv_1): Conv3d(32, 128, kernel_size=(1, 1, 1), stride=(1, 1, 1))\n",
      "      (norm_2): BatchNorm3d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (act_2): LeakyReLU(negative_slope=0.1, inplace=True)\n",
      "      (conv_2): Conv3d(128, 32, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))\n",
      "    )\n",
      "    (denseblock1_layer2): ConvBlock(\n",
      "      (norm_1): BatchNorm3d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (act_1): LeakyReLU(negative_slope=0.1, inplace=True)\n",
      "      (conv_1): Conv3d(64, 128, kernel_size=(1, 1, 1), stride=(1, 1, 1))\n",
      "      (norm_2): BatchNorm3d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (act_2): LeakyReLU(negative_slope=0.1, inplace=True)\n",
      "      (conv_2): Conv3d(128, 32, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))\n",
      "    )\n",
      "    (transition1): TransmitBlock(\n",
      "      (norm): BatchNorm3d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (act): LeakyReLU(negative_slope=0.1, inplace=True)\n",
      "      (conv): Conv3d(96, 48, kernel_size=(1, 1, 1), stride=(1, 1, 1))\n",
      "      (pool): AvgPool3d(kernel_size=2, stride=2, padding=0)\n",
      "    )\n",
      "    (denseblock2_layer1): ConvBlock(\n",
      "      (norm_1): BatchNorm3d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (act_1): LeakyReLU(negative_slope=0.1, inplace=True)\n",
      "      (conv_1): Conv3d(48, 128, kernel_size=(1, 1, 1), stride=(1, 1, 1))\n",
      "      (norm_2): BatchNorm3d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (act_2): LeakyReLU(negative_slope=0.1, inplace=True)\n",
      "      (conv_2): Conv3d(128, 32, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))\n",
      "    )\n",
      "    (denseblock2_layer2): ConvBlock(\n",
      "      (norm_1): BatchNorm3d(80, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (act_1): LeakyReLU(negative_slope=0.1, inplace=True)\n",
      "      (conv_1): Conv3d(80, 128, kernel_size=(1, 1, 1), stride=(1, 1, 1))\n",
      "      (norm_2): BatchNorm3d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (act_2): LeakyReLU(negative_slope=0.1, inplace=True)\n",
      "      (conv_2): Conv3d(128, 32, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))\n",
      "    )\n",
      "    (transition2): TransmitBlock(\n",
      "      (norm): BatchNorm3d(112, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (act): LeakyReLU(negative_slope=0.1, inplace=True)\n",
      "      (conv): Conv3d(112, 56, kernel_size=(1, 1, 1), stride=(1, 1, 1))\n",
      "      (pool): AvgPool3d(kernel_size=2, stride=2, padding=0)\n",
      "    )\n",
      "    (denseblock3_layer1): ConvBlock(\n",
      "      (norm_1): BatchNorm3d(56, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (act_1): LeakyReLU(negative_slope=0.1, inplace=True)\n",
      "      (conv_1): Conv3d(56, 128, kernel_size=(1, 1, 1), stride=(1, 1, 1))\n",
      "      (norm_2): BatchNorm3d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (act_2): LeakyReLU(negative_slope=0.1, inplace=True)\n",
      "      (conv_2): Conv3d(128, 32, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))\n",
      "    )\n",
      "    (denseblock3_layer2): ConvBlock(\n",
      "      (norm_1): BatchNorm3d(88, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (act_1): LeakyReLU(negative_slope=0.1, inplace=True)\n",
      "      (conv_1): Conv3d(88, 128, kernel_size=(1, 1, 1), stride=(1, 1, 1))\n",
      "      (norm_2): BatchNorm3d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (act_2): LeakyReLU(negative_slope=0.1, inplace=True)\n",
      "      (conv_2): Conv3d(128, 32, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))\n",
      "    )\n",
      "    (transition3): TransmitBlock(\n",
      "      (norm): BatchNorm3d(120, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (act): LeakyReLU(negative_slope=0.1, inplace=True)\n",
      "    )\n",
      "  )\n",
      "  (classifier): Linear(in_features=120, out_features=2, bias=True)\n",
      ")\n",
      "Random initialized.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/vivi/anaconda3/envs/3dunet/lib/python3.7/site-packages/ipykernel_launcher.py:30: FutureWarning: get_value is deprecated and will be removed in a future release. Please use .at[] or .iat[] accessors instead\n"
     ]
    }
   ],
   "source": [
    "# '''[begin] load the train and validation dataset:'''\n",
    "# import numpy as np\n",
    "# # use np.load to import data, devide dataset into 2 parts: train_data & validation_data:\n",
    "# # credit: cheez & Matthew Kerian\n",
    "# # link: https://stackoverflow.com/questions/55890813/how-to-fix-object-arrays-cannot-be-loaded-when-allow-pickle-false-for-imdb-loa/56243777\n",
    "# '''use older to successfully load the data:'''\n",
    "# # save np.load\n",
    "# np_load_old = np.load\n",
    "\n",
    "# # modify the default parameters of np.load\n",
    "# np.load = lambda *a,**k: np_load_old(*a, allow_pickle=True, **k)\n",
    "\n",
    "# data_ineed = np.load('muchdata.npy')\n",
    "\n",
    "# # restore np.load for future normal usage\n",
    "# np.load = np_load_old\n",
    "\n",
    "# # If you are working with the basic sample data, use maybe 2 instead of 100 here... you don't have enough data to really do this\n",
    "# train_data = data_ineed[:-60]\n",
    "# validation_data = data_ineed[-60:]\n",
    "# '''[end] load the train and validation dataset:'''\n",
    "\n",
    "model = densenet3d(with_segment=False, use_memonger=True).cuda()\n",
    "# model = nn.DataParallel(model, device_ids=[0, 1])\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "criterion = criterion.cuda()\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "data_train = ClfDataset(train=True)\n",
    "train_data_loader = DataLoader(dataset=data_train, batch_size=32, shuffle=True)\n",
    "dev_data_loader = DataLoader(dataset=data_train, batch_size=32, shuffle=False)\n",
    "data_test = ClfDataset(train=False)\n",
    "test_data_loader = DataLoader(dataset=data_test, batch_size=32, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for epoch in range(4):  # loop over the dataset multiple times\n",
    "#     running_loss = 0.0\n",
    "#     for inputs, labels in train_data_loader:\n",
    "#         # get the inputs; data is a list of [inputs, labels]\n",
    "#         # inputs: tensor(100*(100*100)), label\n",
    "#         inputs_numpy, labels = data\n",
    "#         inputs = torch.from_numpy(inputs_numpy)\n",
    "#         inputs = inputs.unsqueeze(0)\n",
    "#         inputs = inputs.unsqueeze(0)\n",
    "#         inputs = inputs.cuda()\n",
    "# #         inputs = inputs.float()\n",
    "#         labels = torch.from_numpy(np.asarray(labels)).float()\n",
    "#         labels = labels.long() #credit: ptrblck; link: https://discuss.pytorch.org/t/runtimeerror-expected-object-of-scalar-type-long-but-got-scalar-type-float-when-using-crossentropyloss/30542/4\n",
    "#         labels = labels.unsqueeze(0)\n",
    "\n",
    "#         labels = labels.cuda()\n",
    "#         # zero the parameter gradients\n",
    "#         optimizer.zero_grad()\n",
    "\n",
    "#         # forward + backward + optimize\n",
    "#         outputs = model(inputs)\n",
    "        \n",
    "#         loss = criterion(outputs, labels)\n",
    "#         loss.backward()\n",
    "#         optimizer.step()\n",
    "\n",
    "#         # print statistics\n",
    "#         running_loss += loss.item()\n",
    "#         if i % 20 == 19:    # print every 2000 mini-batches\n",
    "#             print('[%d, %5d] loss: %.3f' %\n",
    "#                   (epoch + 1, i + 1, running_loss / 20))\n",
    "#             running_loss = 0.0\n",
    "\n",
    "# print('Finished Training')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 15/15 [00:08<00:00,  1.84it/s]\n",
      "  0%|          | 0/15 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1] loss: 0.027\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 15/15 [00:07<00:00,  1.88it/s]\n",
      "  0%|          | 0/15 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2] loss: 0.026\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 15/15 [00:08<00:00,  1.87it/s]\n",
      "  0%|          | 0/15 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[3] loss: 0.026\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 15/15 [00:08<00:00,  1.87it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[4] loss: 0.025\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(4):  # loop over the dataset multiple times\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    for index, (inputs, labels) in enumerate(tqdm(train_data_loader)):\n",
    "        # zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        inputs, labels = inputs.cuda(), labels.cuda()\n",
    "        \n",
    "        # forward + backward + optimize\n",
    "        inputs = inputs.unsqueeze(dim=1).float()\n",
    "        \n",
    "        inputs = F.interpolate(inputs, size=[32,32,32],mode='trilinear',align_corners=False)\n",
    "        \n",
    "        outputs = model(inputs)\n",
    "        \n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        # print statistics\n",
    "        running_loss += loss.item()\n",
    "        \n",
    "\n",
    "    print('[%d] loss: %.3f' %\n",
    "        (epoch + 1, running_loss / 400))\n",
    "    running_loss = 0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.eval()\n",
    "# dev_loss = 0\n",
    "# predict_value = torch.zeros(1)\n",
    "# true_value = torch.zeros(1)\n",
    "\n",
    "# for index, (inputs, labels) in enumerate(tqdm(train_data_loader)):\n",
    "#     if index<=10:\n",
    "#         continue\n",
    "    \n",
    "#     inputs, labels = inputs.cuda(), labels.cuda()\n",
    "        \n",
    "#         # forward + backward + optimize\n",
    "#     inputs = inputs.unsqueeze(dim=1).float()\n",
    "        \n",
    "#     inputs = F.interpolate(inputs, size=[32,32,32],mode='trilinear',align_corners=False)\n",
    "        \n",
    "#     outputs = model(inputs)\n",
    "    \n",
    "#     test_value = F.softmax(outputs)\n",
    "#     test_value_1 = test_value[:,1]\n",
    "#     test_value_1 = test_value_1.cpu()\n",
    "#     predict_value = torch.cat((predict_value,test_value_1))\n",
    "    \n",
    "#     test_value_2 = labels\n",
    "#     test_value_2 = test_value_2.cpu().float()\n",
    "#     true_value = torch.cat((true_value, test_value_2))\n",
    "# #     loss = criterion(outputs, labels)    \n",
    "# #     dev_loss+=loss.item()\n",
    "    \n",
    "# # dev_loss = dev_loss/64\n",
    "# # print(\"%.4f\" %dev_loss)\n",
    "\n",
    "# predict_value = predict_value.detach().numpy()\n",
    "# true_value = true_value.numpy()\n",
    "\n",
    "# from sklearn.metrics import roc_auc_score\n",
    "# roc_auc_score(true_value, predict_value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH = './my_new.pth'\n",
    "torch.save(model.state_dict(), PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/4 [00:00<?, ?it/s]/home/vivi/anaconda3/envs/3dunet/lib/python3.7/site-packages/ipykernel_launcher.py:16: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  app.launch_new_instance()\n",
      "100%|██████████| 4/4 [00:00<00:00,  5.76it/s]\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "dev_loss = 0\n",
    "predict_value = torch.zeros(1)\n",
    "# true_value = torch.zeros(1)\n",
    "\n",
    "for _, inputs in enumerate(tqdm(test_data_loader)):\n",
    "    \n",
    "    inputs = inputs.cuda()\n",
    "        # forward + backward + optimize\n",
    "    inputs = inputs.unsqueeze(dim=1).float()\n",
    "        \n",
    "    inputs = F.interpolate(inputs, size=[32,32,32],mode='trilinear',align_corners=False)\n",
    "        \n",
    "    outputs = model(inputs)\n",
    "    \n",
    "    test_value = F.softmax(outputs)\n",
    "    test_value_1 = test_value[:,1]\n",
    "    test_value_1 = test_value_1.cpu()\n",
    "    predict_value = torch.cat((predict_value,test_value_1))\n",
    "    \n",
    "#     test_value_2 = labels\n",
    "#     test_value_2 = test_value_2.cpu().float()\n",
    "#     true_value = torch.cat((true_value, test_value_2))\n",
    "\n",
    "\n",
    "predict_value = predict_value.detach().numpy()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.        , 0.5051803 , 0.41650194, 0.41142955, 0.4414225 ,\n",
       "       0.39096144, 0.6262839 , 0.46014047, 0.47756916, 0.6277847 ,\n",
       "       0.56746083, 0.5730454 , 0.63167346, 0.5854904 , 0.6748885 ,\n",
       "       0.42523515, 0.4776123 , 0.6514896 , 0.4976394 , 0.63768333,\n",
       "       0.5131856 , 0.67272997, 0.651423  , 0.5074982 , 0.8104575 ,\n",
       "       0.30000716, 0.4475833 , 0.54921204, 0.26285955, 0.50494415,\n",
       "       0.50495344, 0.50302225, 0.5340415 , 0.4590413 , 0.6359774 ,\n",
       "       0.31955597, 0.4860624 , 0.5199765 , 0.4721789 , 0.48994786,\n",
       "       0.62291473, 0.8040738 , 0.51900595, 0.45329604, 0.66672003,\n",
       "       0.6819406 , 0.483706  , 0.57242197, 0.60705864, 0.27008247,\n",
       "       0.35926816, 0.45925465, 0.756868  , 0.5952081 , 0.6619591 ,\n",
       "       0.4908233 , 0.419681  , 0.524609  , 0.7308577 , 0.47272992,\n",
       "       0.6301511 , 0.4056931 , 0.4930415 , 0.6177794 , 0.73055816,\n",
       "       0.5723529 , 0.694036  , 0.6958936 , 0.5558414 , 0.50139475,\n",
       "       0.5956559 , 0.55584854, 0.6410049 , 0.47101825, 0.45950085,\n",
       "       0.34878775, 0.51949286, 0.47416392, 0.5673299 , 0.61993563,\n",
       "       0.29746845, 0.73972005, 0.5377659 , 0.6194812 , 0.4464901 ,\n",
       "       0.4247208 , 0.633069  , 0.72063226, 0.6907027 , 0.38615233,\n",
       "       0.6519978 , 0.5550086 , 0.74935716, 0.5651362 , 0.43018538,\n",
       "       0.47770134, 0.42284715, 0.51558137, 0.54201156, 0.4672915 ,\n",
       "       0.73891044, 0.4089105 , 0.4478751 , 0.4984161 , 0.41195503,\n",
       "       0.62465805, 0.6081647 , 0.43988758, 0.6321048 , 0.54148495,\n",
       "       0.42606592, 0.5210025 , 0.33475706, 0.42585528, 0.3780813 ,\n",
       "       0.6033618 , 0.34330028, 0.6880235 ], dtype=float32)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "118"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict_value.size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "97"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ineed.size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.savetxt('new.csv', predict_value, delimiter = ',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
