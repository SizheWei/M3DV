{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Credit: [Rongyao Fang](https://github.com/rongyaofang)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.checkpoint import checkpoint\n",
    "device=torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "PARAMS = {\n",
    "    \"input_channels\": 1,\n",
    "    \"output_channels\": 2,\n",
    "    \"input_transform_fn\": lambda x: x / 128. - 1.,\n",
    "    \"input_conv_channels\": 32,\n",
    "    'down_structure': [2,2,2],\n",
    "#     'down_structure': [3, 8, 4],\n",
    "    'activation_fn': lambda: nn.LeakyReLU(0.1, inplace=True),\n",
    "    # \"normalization_fn\": lambda c: nn.GroupNorm(num_groups=4, num_channels=c),\n",
    "    \"normalization_fn\": lambda c: nn.BatchNorm3d(num_features=c),\n",
    "    \"drop_rate\": 0,\n",
    "    'growth_rate': 32,\n",
    "    'bottleneck': 4,\n",
    "    'compression': 2,\n",
    "    'use_memonger': True  # ~2.2x memory efficiency (batch_size), 25%~30% slower on GTX 1080.\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "def densenet3d(with_segment=False, snapshot=None, **kwargs):\n",
    "    for k, v in kwargs.items():\n",
    "        assert k in PARAMS\n",
    "        PARAMS[k] = v\n",
    "    print(\"Model hyper-parameters:\", PARAMS)\n",
    "    if with_segment:\n",
    "        model = DenseSharp()\n",
    "        print(\"Using DenseSharp model.\")\n",
    "    else:\n",
    "        model = DenseNet()\n",
    "        print(\"Using DenseNet model.\")\n",
    "    print(model)\n",
    "    if snapshot is None:\n",
    "        initialize(model.modules())\n",
    "        print(\"Random initialized.\")\n",
    "    else:\n",
    "        state_dict = torch.load(snapshot)\n",
    "        model.load_state_dict(state_dict)\n",
    "        print(\"Load weights from `%s`,\" % snapshot)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize(modules):\n",
    "    for m in modules:\n",
    "        if isinstance(m, nn.Conv3d) or isinstance(m, nn.ConvTranspose3d):\n",
    "            nn.init.kaiming_uniform_(m.weight, mode='fan_in')\n",
    "        elif isinstance(m, nn.Linear):\n",
    "            nn.init.kaiming_uniform_(m.weight, mode='fan_in')\n",
    "            m.bias.data.zero_()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvBlock(nn.Sequential):\n",
    "    def __init__(self, in_channels):\n",
    "        super(ConvBlock, self).__init__()\n",
    "\n",
    "        growth_rate = PARAMS['growth_rate']\n",
    "        bottleneck = PARAMS['bottleneck']\n",
    "        activation_fn = PARAMS['activation_fn']\n",
    "        normalization_fn = PARAMS['normalization_fn']\n",
    "\n",
    "        self.in_channels = in_channels\n",
    "        self.growth_rate = growth_rate\n",
    "        self.use_memonger = PARAMS['use_memonger']\n",
    "        self.drop_rate = PARAMS['drop_rate']\n",
    "\n",
    "        # TODO: consider bias term in conv with GN\n",
    "        self.add_module('norm_1', normalization_fn(in_channels))\n",
    "        self.add_module('act_1', activation_fn())\n",
    "        self.add_module('conv_1', nn.Conv3d(in_channels, bottleneck * growth_rate, kernel_size=1, stride=1,\n",
    "                                            padding=0, bias=True))\n",
    "\n",
    "        self.add_module('norm_2', normalization_fn(bottleneck * growth_rate))\n",
    "        self.add_module('act_2', activation_fn())\n",
    "        self.add_module('conv_2', nn.Conv3d(bottleneck * growth_rate, growth_rate, kernel_size=3, stride=1,\n",
    "                                            padding=1, bias=True))\n",
    "\n",
    "    def forward(self, x):\n",
    "        super_forward = super(ConvBlock, self).forward\n",
    "        if self.use_memonger:\n",
    "            new_features = checkpoint(super_forward, x)\n",
    "        else:\n",
    "            new_features = super_forward(x)\n",
    "        if self.drop_rate > 0:\n",
    "            new_features = F.dropout(new_features, p=self.drop_rate, training=self.training)\n",
    "        return torch.cat([x, new_features], 1)\n",
    "\n",
    "    @property\n",
    "    def out_channels(self):\n",
    "        return self.in_channels + self.growth_rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransmitBlock(nn.Sequential):\n",
    "    def __init__(self, in_channels, is_last_block):\n",
    "        super(TransmitBlock, self).__init__()\n",
    "\n",
    "        activation_fn = PARAMS['activation_fn']\n",
    "        normalization_fn = PARAMS['normalization_fn']\n",
    "        compression = PARAMS['compression']\n",
    "\n",
    "        # print(\"in_channels: %s, compression: %s\" % (in_channels, compression))\n",
    "        assert in_channels % compression == 0\n",
    "\n",
    "        self.in_channels = in_channels\n",
    "        self.compression = compression\n",
    "\n",
    "        self.add_module('norm', normalization_fn(in_channels))\n",
    "        self.add_module('act', activation_fn())\n",
    "        if not is_last_block:\n",
    "            self.add_module('conv', nn.Conv3d(in_channels, in_channels // compression, kernel_size=(1, 1, 1),\n",
    "                                              stride=1, padding=0, bias=True))\n",
    "            self.add_module('pool', nn.AvgPool3d(kernel_size=2, stride=2, padding=0))\n",
    "        else:\n",
    "            self.compression = 1\n",
    "\n",
    "    @property\n",
    "    def out_channels(self):\n",
    "        return self.in_channels // self.compression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Lambda(nn.Module):\n",
    "    def __init__(self, lambda_fn):\n",
    "        super(Lambda, self).__init__()\n",
    "        self.lambda_fn = lambda_fn\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.lambda_fn(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DenseNet(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "\n",
    "        super(DenseNet, self).__init__()\n",
    "\n",
    "        input_channels = PARAMS['input_channels']\n",
    "        input_transform_fn = PARAMS['input_transform_fn']\n",
    "        input_conv_channels = PARAMS['input_conv_channels']\n",
    "        normalization_fn = PARAMS['normalization_fn']\n",
    "        activation_fn = PARAMS['activation_fn']\n",
    "        down_structure = PARAMS['down_structure']\n",
    "        output_channels = PARAMS['output_channels']\n",
    "\n",
    "        self.features = nn.Sequential()\n",
    "        if input_transform_fn is not None:\n",
    "            self.features.add_module(\"input_transform\", Lambda(input_transform_fn))\n",
    "        self.features.add_module(\"init_conv\", nn.Conv3d(input_channels, input_conv_channels, kernel_size=3,\n",
    "                                                        stride=1, padding=1, bias=True))\n",
    "        self.features.add_module(\"init_norm\", normalization_fn(input_conv_channels))\n",
    "        self.features.add_module(\"init_act\", activation_fn())\n",
    "\n",
    "        channels = input_conv_channels\n",
    "        for i, num_layers in enumerate(down_structure):\n",
    "            for j in range(num_layers):\n",
    "                conv_layer = ConvBlock(channels)\n",
    "                self.features.add_module('denseblock{}_layer{}'.format(i + 1, j + 1), conv_layer)\n",
    "                channels = conv_layer.out_channels\n",
    "                # print(i, j, channels)\n",
    "\n",
    "            trans_layer = TransmitBlock(channels, is_last_block=i == len(down_structure) - 1)\n",
    "            self.features.add_module('transition%d' % (i + 1), trans_layer)\n",
    "            channels = trans_layer.out_channels\n",
    "\n",
    "        self.classifier = nn.Linear(channels, output_channels)\n",
    "\n",
    "    def forward(self, x, **return_opts):\n",
    "        # o = x\n",
    "        # for i, layer in enumerate(self.features):\n",
    "        #     o = layer(o)\n",
    "        #     print(i, layer, o.shape)\n",
    "\n",
    "        batch_size, _, d, h, w = x.size()\n",
    "\n",
    "        features = self.features(x)\n",
    "        pooled = F.adaptive_avg_pool3d(features, 1).view(batch_size, -1)\n",
    "        scores = self.classifier(pooled)\n",
    "\n",
    "        if len(return_opts) == 0:\n",
    "            return scores\n",
    "\n",
    "        # return other features in one forward\n",
    "        for opt in return_opts:\n",
    "            assert opt in {\"return_features\", \"return_cam\"}\n",
    "        # print(return_opts)\n",
    "\n",
    "        ret = dict(scores=scores)\n",
    "\n",
    "        if 'return_features' in return_opts and return_opts['return_features']:\n",
    "            ret['features'] = features\n",
    "\n",
    "        if 'return_cam' in return_opts and return_opts['return_cam']:\n",
    "            weight = self.classifier.weight.unsqueeze(-1).unsqueeze(-1).unsqueeze(-1)\n",
    "            bias = self.classifier.bias\n",
    "            cam_raw = F.conv3d(features, weight, bias)\n",
    "            cam = F.interpolate(cam_raw, size=(d, h, w), mode='trilinear', align_corners=True)\n",
    "            ret['cam'] = F.softmax(cam, dim=1)\n",
    "            ret['cam_raw'] = F.softmax(cam_raw, dim=1)\n",
    "        return ret"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "import random\n",
    "import torch\n",
    "import os\n",
    "import torch.nn as nn\n",
    "from tqdm import tqdm\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "class ClfDataset(Dataset):\n",
    "\n",
    "    def __init__(self, train=True):\n",
    "        self.train = train\n",
    "        data_dir = './dataset/'\n",
    "        # choose the dataset\n",
    "        patients_train = os.listdir(data_dir+'train_val/')\n",
    "        patients_test = os.listdir(data_dir+'test/')\n",
    "        labels_df = pd.read_csv('./dataset/info.csv',index_col=0)\n",
    "\n",
    "        self.data_train = []\n",
    "        self.data_test = []\n",
    "        self.label = []\n",
    "\n",
    "        for num, patient in enumerate(patients_train):\n",
    "            patient_name = patient[0:-4]\n",
    "            label = labels_df.get_value(patient_name, 'lable')\n",
    "            path = data_dir + 'train_val/' + patient\n",
    "            img_data = np.load(path)\n",
    "            voxel = img_data['voxel'].astype(np.int32)\n",
    "            self.data_train.append(voxel)\n",
    "            self.label.append(label)\n",
    "\n",
    "        for num, patient in enumerate(patients_test):\n",
    "            path = data_dir + 'test/' + patient\n",
    "            img_data = np.load(path)\n",
    "            voxel = img_data['voxel'].astype(np.int32)\n",
    "            self.data_test.append(voxel)\n",
    "    \n",
    "    def __getitem__(self, item):\n",
    "        if self.train:\n",
    "            patient_data = self.data_train[item]\n",
    "            patient_label = self.label[item]\n",
    "            return patient_data, patient_label\n",
    "        else:\n",
    "            patient_data = self.data_test[item]\n",
    "            return patient_data\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model hyper-parameters: {'input_channels': 1, 'output_channels': 2, 'input_transform_fn': <function <lambda> at 0x7f13646b7d08>, 'input_conv_channels': 32, 'down_structure': [2, 2, 2], 'activation_fn': <function <lambda> at 0x7f13646b7f28>, 'normalization_fn': <function <lambda> at 0x7f13646b72f0>, 'drop_rate': 0, 'growth_rate': 32, 'bottleneck': 4, 'compression': 2, 'use_memonger': True}\n",
      "Using DenseNet model.\n",
      "DenseNet(\n",
      "  (features): Sequential(\n",
      "    (input_transform): Lambda()\n",
      "    (init_conv): Conv3d(1, 32, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))\n",
      "    (init_norm): BatchNorm3d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (init_act): LeakyReLU(negative_slope=0.1, inplace=True)\n",
      "    (denseblock1_layer1): ConvBlock(\n",
      "      (norm_1): BatchNorm3d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (act_1): LeakyReLU(negative_slope=0.1, inplace=True)\n",
      "      (conv_1): Conv3d(32, 128, kernel_size=(1, 1, 1), stride=(1, 1, 1))\n",
      "      (norm_2): BatchNorm3d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (act_2): LeakyReLU(negative_slope=0.1, inplace=True)\n",
      "      (conv_2): Conv3d(128, 32, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))\n",
      "    )\n",
      "    (denseblock1_layer2): ConvBlock(\n",
      "      (norm_1): BatchNorm3d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (act_1): LeakyReLU(negative_slope=0.1, inplace=True)\n",
      "      (conv_1): Conv3d(64, 128, kernel_size=(1, 1, 1), stride=(1, 1, 1))\n",
      "      (norm_2): BatchNorm3d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (act_2): LeakyReLU(negative_slope=0.1, inplace=True)\n",
      "      (conv_2): Conv3d(128, 32, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))\n",
      "    )\n",
      "    (transition1): TransmitBlock(\n",
      "      (norm): BatchNorm3d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (act): LeakyReLU(negative_slope=0.1, inplace=True)\n",
      "      (conv): Conv3d(96, 48, kernel_size=(1, 1, 1), stride=(1, 1, 1))\n",
      "      (pool): AvgPool3d(kernel_size=2, stride=2, padding=0)\n",
      "    )\n",
      "    (denseblock2_layer1): ConvBlock(\n",
      "      (norm_1): BatchNorm3d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (act_1): LeakyReLU(negative_slope=0.1, inplace=True)\n",
      "      (conv_1): Conv3d(48, 128, kernel_size=(1, 1, 1), stride=(1, 1, 1))\n",
      "      (norm_2): BatchNorm3d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (act_2): LeakyReLU(negative_slope=0.1, inplace=True)\n",
      "      (conv_2): Conv3d(128, 32, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))\n",
      "    )\n",
      "    (denseblock2_layer2): ConvBlock(\n",
      "      (norm_1): BatchNorm3d(80, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (act_1): LeakyReLU(negative_slope=0.1, inplace=True)\n",
      "      (conv_1): Conv3d(80, 128, kernel_size=(1, 1, 1), stride=(1, 1, 1))\n",
      "      (norm_2): BatchNorm3d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (act_2): LeakyReLU(negative_slope=0.1, inplace=True)\n",
      "      (conv_2): Conv3d(128, 32, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))\n",
      "    )\n",
      "    (transition2): TransmitBlock(\n",
      "      (norm): BatchNorm3d(112, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (act): LeakyReLU(negative_slope=0.1, inplace=True)\n",
      "      (conv): Conv3d(112, 56, kernel_size=(1, 1, 1), stride=(1, 1, 1))\n",
      "      (pool): AvgPool3d(kernel_size=2, stride=2, padding=0)\n",
      "    )\n",
      "    (denseblock3_layer1): ConvBlock(\n",
      "      (norm_1): BatchNorm3d(56, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (act_1): LeakyReLU(negative_slope=0.1, inplace=True)\n",
      "      (conv_1): Conv3d(56, 128, kernel_size=(1, 1, 1), stride=(1, 1, 1))\n",
      "      (norm_2): BatchNorm3d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (act_2): LeakyReLU(negative_slope=0.1, inplace=True)\n",
      "      (conv_2): Conv3d(128, 32, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))\n",
      "    )\n",
      "    (denseblock3_layer2): ConvBlock(\n",
      "      (norm_1): BatchNorm3d(88, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (act_1): LeakyReLU(negative_slope=0.1, inplace=True)\n",
      "      (conv_1): Conv3d(88, 128, kernel_size=(1, 1, 1), stride=(1, 1, 1))\n",
      "      (norm_2): BatchNorm3d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (act_2): LeakyReLU(negative_slope=0.1, inplace=True)\n",
      "      (conv_2): Conv3d(128, 32, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))\n",
      "    )\n",
      "    (transition3): TransmitBlock(\n",
      "      (norm): BatchNorm3d(120, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (act): LeakyReLU(negative_slope=0.1, inplace=True)\n",
      "    )\n",
      "  )\n",
      "  (classifier): Linear(in_features=120, out_features=2, bias=True)\n",
      ")\n",
      "Random initialized.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/vivi/anaconda3/envs/3dunet/lib/python3.7/site-packages/ipykernel_launcher.py:28: FutureWarning: get_value is deprecated and will be removed in a future release. Please use .at[] or .iat[] accessors instead\n"
     ]
    }
   ],
   "source": [
    "# '''[begin] load the train and validation dataset:'''\n",
    "# import numpy as np\n",
    "# # use np.load to import data, devide dataset into 2 parts: train_data & validation_data:\n",
    "# # credit: cheez & Matthew Kerian\n",
    "# # link: https://stackoverflow.com/questions/55890813/how-to-fix-object-arrays-cannot-be-loaded-when-allow-pickle-false-for-imdb-loa/56243777\n",
    "# '''use older to successfully load the data:'''\n",
    "# # save np.load\n",
    "# np_load_old = np.load\n",
    "\n",
    "# # modify the default parameters of np.load\n",
    "# np.load = lambda *a,**k: np_load_old(*a, allow_pickle=True, **k)\n",
    "\n",
    "# data_ineed = np.load('muchdata.npy')\n",
    "\n",
    "# # restore np.load for future normal usage\n",
    "# np.load = np_load_old\n",
    "\n",
    "# # If you are working with the basic sample data, use maybe 2 instead of 100 here... you don't have enough data to really do this\n",
    "# train_data = data_ineed[:-60]\n",
    "# validation_data = data_ineed[-60:]\n",
    "# '''[end] load the train and validation dataset:'''\n",
    "\n",
    "model = densenet3d(with_segment=False, use_memonger=True).cuda()\n",
    "# model = nn.DataParallel(model, device_ids=[0, 1])\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "criterion = criterion.cuda()\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "data_train = ClfDataset(train=True)\n",
    "train_data_loader = DataLoader(dataset=data_train, batch_size=32, shuffle=True)\n",
    "dev_data_loader = DataLoader(dataset=data_train, batch_size=32, shuffle=False)\n",
    "data_test = ClfDataset(train=False)\n",
    "test_data_loader = DataLoader(dataset=data_test, batch_size=32, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for epoch in range(4):  # loop over the dataset multiple times\n",
    "#     running_loss = 0.0\n",
    "#     for inputs, labels in train_data_loader:\n",
    "#         # get the inputs; data is a list of [inputs, labels]\n",
    "#         # inputs: tensor(100*(100*100)), label\n",
    "#         inputs_numpy, labels = data\n",
    "#         inputs = torch.from_numpy(inputs_numpy)\n",
    "#         inputs = inputs.unsqueeze(0)\n",
    "#         inputs = inputs.unsqueeze(0)\n",
    "#         inputs = inputs.cuda()\n",
    "# #         inputs = inputs.float()\n",
    "#         labels = torch.from_numpy(np.asarray(labels)).float()\n",
    "#         labels = labels.long() #credit: ptrblck; link: https://discuss.pytorch.org/t/runtimeerror-expected-object-of-scalar-type-long-but-got-scalar-type-float-when-using-crossentropyloss/30542/4\n",
    "#         labels = labels.unsqueeze(0)\n",
    "\n",
    "#         labels = labels.cuda()\n",
    "#         # zero the parameter gradients\n",
    "#         optimizer.zero_grad()\n",
    "\n",
    "#         # forward + backward + optimize\n",
    "#         outputs = model(inputs)\n",
    "        \n",
    "#         loss = criterion(outputs, labels)\n",
    "#         loss.backward()\n",
    "#         optimizer.step()\n",
    "\n",
    "#         # print statistics\n",
    "#         running_loss += loss.item()\n",
    "#         if i % 20 == 19:    # print every 2000 mini-batches\n",
    "#             print('[%d, %5d] loss: %.3f' %\n",
    "#                   (epoch + 1, i + 1, running_loss / 20))\n",
    "#             running_loss = 0.0\n",
    "\n",
    "# print('Finished Training')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/8 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "CUDA out of memory. Tried to allocate 768.00 MiB (GPU 0; 10.76 GiB total capacity; 6.75 GiB already allocated; 77.56 MiB free; 3.16 GiB cached)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-113-a07be1653db1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/3dunet/lib/python3.7/site-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[1;32m    164\u001b[0m                 \u001b[0mproducts\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0mDefaults\u001b[0m \u001b[0mto\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    165\u001b[0m         \"\"\"\n\u001b[0;32m--> 166\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    167\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    168\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/3dunet/lib/python3.7/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[1;32m     97\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[1;32m     98\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 99\u001b[0;31m         allow_unreachable=True)  # allow_unreachable flag\n\u001b[0m\u001b[1;32m    100\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    101\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: CUDA out of memory. Tried to allocate 768.00 MiB (GPU 0; 10.76 GiB total capacity; 6.75 GiB already allocated; 77.56 MiB free; 3.16 GiB cached)"
     ]
    }
   ],
   "source": [
    "for epoch in range(4):  # loop over the dataset multiple times\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    for index, (inputs, labels) in enumerate(tqdm(train_data_loader)):\n",
    "        # zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        inputs, labels = inputs.cuda(), labels.cuda()\n",
    "        \n",
    "        # forward + backward + optimize\n",
    "        inputs = inputs.unsqueeze(dim=1).float()\n",
    "        \n",
    "        inputs = F.interpolate(inputs, size=[32,32,32],mode='trilinear',align_corners=False)\n",
    "        \n",
    "        outputs = model(inputs)\n",
    "        \n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        # print statistics\n",
    "        running_loss += loss.item()\n",
    "        \n",
    "        if index==10:\n",
    "            break\n",
    "\n",
    "    print('[%d] loss: %.3f' %\n",
    "        (epoch + 1, running_loss / 400))\n",
    "    running_loss = 0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "dev_loss = 0\n",
    "predict_value = torch.zeros(1)\n",
    "true_value = torch.zeros(1)\n",
    "\n",
    "for index, (inputs, labels) in enumerate(tqdm(train_data_loader)):\n",
    "    if index<=10:\n",
    "        continue\n",
    "    \n",
    "    inputs, labels = inputs.cuda(), labels.cuda()\n",
    "        \n",
    "        # forward + backward + optimize\n",
    "    inputs = inputs.unsqueeze(dim=1).float()\n",
    "        \n",
    "    inputs = F.interpolate(inputs, size=[32,32,32],mode='trilinear',align_corners=False)\n",
    "        \n",
    "    outputs = model(inputs)\n",
    "    \n",
    "    test_value = F.softmax(outputs)\n",
    "    test_value_1 = test_value[:,1]\n",
    "    test_value_1 = test_value_1.cpu()\n",
    "    predict_value = torch.cat((predict_value,test_value_1))\n",
    "    \n",
    "    test_value_2 = labels\n",
    "    test_value_2 = test_value_2.cpu().float()\n",
    "    true_value = torch.cat((true_value, test_value_2))\n",
    "#     loss = criterion(outputs, labels)    \n",
    "#     dev_loss+=loss.item()\n",
    "    \n",
    "# dev_loss = dev_loss/64\n",
    "# print(\"%.4f\" %dev_loss)\n",
    "\n",
    "predict_value = predict_value.detach().numpy()\n",
    "true_value = true_value.numpy()\n",
    "\n",
    "from sklearn.metrics import roc_auc_score\n",
    "roc_auc_score(true_value, predict_value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
