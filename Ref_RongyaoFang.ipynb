{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Credit: [Rongyao Fang](https://github.com/rongyaofang)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.checkpoint import checkpoint\n",
    "device=torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "from sklearn.metrics import roc_auc_score\n",
    "# from tensorboardX import SummaryWriter\n",
    "import tensorboard_logger as tb_logger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "PARAMS = {\n",
    "    \"input_channels\": 1,\n",
    "    \"output_channels\": 2,\n",
    "    \"input_transform_fn\": lambda x: x / 128. - 1.,\n",
    "    \"input_conv_channels\": 32,\n",
    "    'down_structure': [2,2,2],\n",
    "#     'down_structure': [3, 8, 4],\n",
    "    'activation_fn': lambda: nn.LeakyReLU(0.1, inplace=True),\n",
    "    # \"normalization_fn\": lambda c: nn.GroupNorm(num_groups=4, num_channels=c),\n",
    "    \"normalization_fn\": lambda c: nn.BatchNorm3d(num_features=c),\n",
    "    \"drop_rate\": 0,\n",
    "    'growth_rate': 32,\n",
    "    'bottleneck': 4,\n",
    "    'compression': 2,\n",
    "    'use_memonger': True  # ~2.2x memory efficiency (batch_size), 25%~30% slower on GTX 1080.\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def densenet3d(with_segment=False, snapshot=None, **kwargs):\n",
    "    for k, v in kwargs.items():\n",
    "        assert k in PARAMS\n",
    "        PARAMS[k] = v\n",
    "    print(\"Model hyper-parameters:\", PARAMS)\n",
    "    if with_segment:\n",
    "        model = DenseSharp()\n",
    "        print(\"Using DenseSharp model.\")\n",
    "    else:\n",
    "        model = DenseNet()\n",
    "        print(\"Using DenseNet model.\")\n",
    "    print(model)\n",
    "    if snapshot is None:\n",
    "        initialize(model.modules())\n",
    "        print(\"Random initialized.\")\n",
    "    else:\n",
    "        state_dict = torch.load(snapshot)\n",
    "        model.load_state_dict(state_dict)\n",
    "        print(\"Load weights from `%s`,\" % snapshot)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize(modules):\n",
    "    for m in modules:\n",
    "        if isinstance(m, nn.Conv3d) or isinstance(m, nn.ConvTranspose3d):\n",
    "            nn.init.kaiming_uniform_(m.weight, mode='fan_in')\n",
    "        elif isinstance(m, nn.Linear):\n",
    "            nn.init.kaiming_uniform_(m.weight, mode='fan_in')\n",
    "            m.bias.data.zero_()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvBlock(nn.Sequential):\n",
    "    def __init__(self, in_channels):\n",
    "        super(ConvBlock, self).__init__()\n",
    "\n",
    "        growth_rate = PARAMS['growth_rate']\n",
    "        bottleneck = PARAMS['bottleneck']\n",
    "        activation_fn = PARAMS['activation_fn']\n",
    "        normalization_fn = PARAMS['normalization_fn']\n",
    "\n",
    "        self.in_channels = in_channels\n",
    "        self.growth_rate = growth_rate\n",
    "        self.use_memonger = PARAMS['use_memonger']\n",
    "        self.drop_rate = PARAMS['drop_rate']\n",
    "\n",
    "        # TODO: consider bias term in conv with GN\n",
    "        self.add_module('norm_1', normalization_fn(in_channels))\n",
    "        self.add_module('act_1', activation_fn())\n",
    "        self.add_module('conv_1', nn.Conv3d(in_channels, bottleneck * growth_rate, kernel_size=1, stride=1,\n",
    "                                            padding=0, bias=True))\n",
    "\n",
    "        self.add_module('norm_2', normalization_fn(bottleneck * growth_rate))\n",
    "        self.add_module('act_2', activation_fn())\n",
    "        self.add_module('conv_2', nn.Conv3d(bottleneck * growth_rate, growth_rate, kernel_size=3, stride=1,\n",
    "                                            padding=1, bias=True))\n",
    "\n",
    "    def forward(self, x):\n",
    "        super_forward = super(ConvBlock, self).forward\n",
    "        if self.use_memonger:\n",
    "            new_features = checkpoint(super_forward, x)\n",
    "        else:\n",
    "            new_features = super_forward(x)\n",
    "        if self.drop_rate > 0:\n",
    "            new_features = F.dropout(new_features, p=self.drop_rate, training=self.training)\n",
    "        return torch.cat([x, new_features], 1)\n",
    "\n",
    "    @property\n",
    "    def out_channels(self):\n",
    "        return self.in_channels + self.growth_rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransmitBlock(nn.Sequential):\n",
    "    def __init__(self, in_channels, is_last_block):\n",
    "        super(TransmitBlock, self).__init__()\n",
    "\n",
    "        activation_fn = PARAMS['activation_fn']\n",
    "        normalization_fn = PARAMS['normalization_fn']\n",
    "        compression = PARAMS['compression']\n",
    "\n",
    "        # print(\"in_channels: %s, compression: %s\" % (in_channels, compression))\n",
    "        assert in_channels % compression == 0\n",
    "\n",
    "        self.in_channels = in_channels\n",
    "        self.compression = compression\n",
    "\n",
    "        self.add_module('norm', normalization_fn(in_channels))\n",
    "        self.add_module('act', activation_fn())\n",
    "        if not is_last_block:\n",
    "            self.add_module('conv', nn.Conv3d(in_channels, in_channels // compression, kernel_size=(1, 1, 1),\n",
    "                                              stride=1, padding=0, bias=True))\n",
    "            self.add_module('pool', nn.AvgPool3d(kernel_size=2, stride=2, padding=0))\n",
    "        else:\n",
    "            self.compression = 1\n",
    "\n",
    "    @property\n",
    "    def out_channels(self):\n",
    "        return self.in_channels // self.compression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Lambda(nn.Module):\n",
    "    def __init__(self, lambda_fn):\n",
    "        super(Lambda, self).__init__()\n",
    "        self.lambda_fn = lambda_fn\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.lambda_fn(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DenseNet(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "\n",
    "        super(DenseNet, self).__init__()\n",
    "\n",
    "        input_channels = PARAMS['input_channels']\n",
    "        input_transform_fn = PARAMS['input_transform_fn']\n",
    "        input_conv_channels = PARAMS['input_conv_channels']\n",
    "        normalization_fn = PARAMS['normalization_fn']\n",
    "        activation_fn = PARAMS['activation_fn']\n",
    "        down_structure = PARAMS['down_structure']\n",
    "        output_channels = PARAMS['output_channels']\n",
    "\n",
    "        self.features = nn.Sequential()\n",
    "        if input_transform_fn is not None:\n",
    "            self.features.add_module(\"input_transform\", Lambda(input_transform_fn))\n",
    "        self.features.add_module(\"init_conv\", nn.Conv3d(input_channels, input_conv_channels, kernel_size=3,\n",
    "                                                        stride=1, padding=1, bias=True))\n",
    "        self.features.add_module(\"init_norm\", normalization_fn(input_conv_channels))\n",
    "        self.features.add_module(\"init_act\", activation_fn())\n",
    "\n",
    "        channels = input_conv_channels\n",
    "        for i, num_layers in enumerate(down_structure):\n",
    "            for j in range(num_layers):\n",
    "                conv_layer = ConvBlock(channels)\n",
    "                self.features.add_module('denseblock{}_layer{}'.format(i + 1, j + 1), conv_layer)\n",
    "                channels = conv_layer.out_channels\n",
    "                # print(i, j, channels)\n",
    "\n",
    "            trans_layer = TransmitBlock(channels, is_last_block=i == len(down_structure) - 1)\n",
    "            self.features.add_module('transition%d' % (i + 1), trans_layer)\n",
    "            channels = trans_layer.out_channels\n",
    "\n",
    "        self.classifier = nn.Linear(channels, output_channels)\n",
    "\n",
    "    def forward(self, x, **return_opts):\n",
    "        # o = x\n",
    "        # for i, layer in enumerate(self.features):\n",
    "        #     o = layer(o)\n",
    "        #     print(i, layer, o.shape)\n",
    "\n",
    "        batch_size, _, d, h, w = x.size()\n",
    "\n",
    "        features = self.features(x)\n",
    "        pooled = F.adaptive_avg_pool3d(features, 1).view(batch_size, -1)\n",
    "        scores = self.classifier(pooled)\n",
    "\n",
    "        if len(return_opts) == 0:\n",
    "            return scores\n",
    "\n",
    "        # return other features in one forward\n",
    "        for opt in return_opts:\n",
    "            assert opt in {\"return_features\", \"return_cam\"}\n",
    "        # print(return_opts)\n",
    "\n",
    "        ret = dict(scores=scores)\n",
    "\n",
    "        if 'return_features' in return_opts and return_opts['return_features']:\n",
    "            ret['features'] = features\n",
    "\n",
    "        if 'return_cam' in return_opts and return_opts['return_cam']:\n",
    "            weight = self.classifier.weight.unsqueeze(-1).unsqueeze(-1).unsqueeze(-1)\n",
    "            bias = self.classifier.bias\n",
    "            cam_raw = F.conv3d(features, weight, bias)\n",
    "            cam = F.interpolate(cam_raw, size=(d, h, w), mode='trilinear', align_corners=True)\n",
    "            ret['cam'] = F.softmax(cam, dim=1)\n",
    "            ret['cam_raw'] = F.softmax(cam_raw, dim=1)\n",
    "        return ret"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "import random\n",
    "import torch\n",
    "import os\n",
    "import torch.nn as nn\n",
    "from tqdm import tqdm\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "class ClfDataset(Dataset):\n",
    "\n",
    "    def __init__(self, train=True):\n",
    "        self.train = train\n",
    "        data_dir = './dataset/'\n",
    "        # choose the dataset\n",
    "        patients_train = os.listdir(data_dir+'train_val/')\n",
    "        patients_train.sort()\n",
    "        patients_test = os.listdir(data_dir+'test/')\n",
    "        patients_test.sort()\n",
    "        labels_df = pd.read_csv('./dataset/info.csv',index_col=0)\n",
    "\n",
    "        self.data_train = []\n",
    "        self.data_test = []\n",
    "        self.label = []\n",
    "\n",
    "        for num, patient in enumerate(patients_train):\n",
    "            patient_name = patient[0:-4]\n",
    "            label = labels_df.get_value(patient_name, 'lable')\n",
    "            path = data_dir + 'train_val/' + patient\n",
    "            img_data = np.load(path)\n",
    "            voxel = img_data['voxel'].astype(np.int32)\n",
    "            voxel_crop = voxel[10:90,10:90,10:90]\n",
    "            self.data_train.append(voxel_crop)\n",
    "            self.label.append(label)\n",
    "\n",
    "        for num, patient in enumerate(patients_test):\n",
    "            path = data_dir + 'test/' + patient\n",
    "            img_data = np.load(path)\n",
    "            voxel = img_data['voxel'].astype(np.int32)\n",
    "            voxel_crop = voxel[10:90,10:90,10:90]\n",
    "            self.data_test.append(voxel_crop)\n",
    "    \n",
    "    def __getitem__(self, item):\n",
    "        if self.train:\n",
    "            patient_data = self.data_train[item]\n",
    "            patient_label = self.label[item]\n",
    "            return patient_data, patient_label\n",
    "        else:\n",
    "            patient_data = self.data_test[item]\n",
    "            return patient_data\n",
    "        \n",
    "    def __len__(self):\n",
    "        if self.train:\n",
    "            return len(self.label)\n",
    "        else:\n",
    "            return len(self.data_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model hyper-parameters: {'input_channels': 1, 'output_channels': 2, 'input_transform_fn': <function <lambda> at 0x7f47ca72a620>, 'input_conv_channels': 32, 'down_structure': [2, 2, 2], 'activation_fn': <function <lambda> at 0x7f47ca72a6a8>, 'normalization_fn': <function <lambda> at 0x7f47ca72a730>, 'drop_rate': 0, 'growth_rate': 32, 'bottleneck': 4, 'compression': 2, 'use_memonger': True}\n",
      "Using DenseNet model.\n",
      "DenseNet(\n",
      "  (features): Sequential(\n",
      "    (input_transform): Lambda()\n",
      "    (init_conv): Conv3d(1, 32, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))\n",
      "    (init_norm): BatchNorm3d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (init_act): LeakyReLU(negative_slope=0.1, inplace=True)\n",
      "    (denseblock1_layer1): ConvBlock(\n",
      "      (norm_1): BatchNorm3d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (act_1): LeakyReLU(negative_slope=0.1, inplace=True)\n",
      "      (conv_1): Conv3d(32, 128, kernel_size=(1, 1, 1), stride=(1, 1, 1))\n",
      "      (norm_2): BatchNorm3d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (act_2): LeakyReLU(negative_slope=0.1, inplace=True)\n",
      "      (conv_2): Conv3d(128, 32, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))\n",
      "    )\n",
      "    (denseblock1_layer2): ConvBlock(\n",
      "      (norm_1): BatchNorm3d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (act_1): LeakyReLU(negative_slope=0.1, inplace=True)\n",
      "      (conv_1): Conv3d(64, 128, kernel_size=(1, 1, 1), stride=(1, 1, 1))\n",
      "      (norm_2): BatchNorm3d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (act_2): LeakyReLU(negative_slope=0.1, inplace=True)\n",
      "      (conv_2): Conv3d(128, 32, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))\n",
      "    )\n",
      "    (transition1): TransmitBlock(\n",
      "      (norm): BatchNorm3d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (act): LeakyReLU(negative_slope=0.1, inplace=True)\n",
      "      (conv): Conv3d(96, 48, kernel_size=(1, 1, 1), stride=(1, 1, 1))\n",
      "      (pool): AvgPool3d(kernel_size=2, stride=2, padding=0)\n",
      "    )\n",
      "    (denseblock2_layer1): ConvBlock(\n",
      "      (norm_1): BatchNorm3d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (act_1): LeakyReLU(negative_slope=0.1, inplace=True)\n",
      "      (conv_1): Conv3d(48, 128, kernel_size=(1, 1, 1), stride=(1, 1, 1))\n",
      "      (norm_2): BatchNorm3d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (act_2): LeakyReLU(negative_slope=0.1, inplace=True)\n",
      "      (conv_2): Conv3d(128, 32, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))\n",
      "    )\n",
      "    (denseblock2_layer2): ConvBlock(\n",
      "      (norm_1): BatchNorm3d(80, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (act_1): LeakyReLU(negative_slope=0.1, inplace=True)\n",
      "      (conv_1): Conv3d(80, 128, kernel_size=(1, 1, 1), stride=(1, 1, 1))\n",
      "      (norm_2): BatchNorm3d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (act_2): LeakyReLU(negative_slope=0.1, inplace=True)\n",
      "      (conv_2): Conv3d(128, 32, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))\n",
      "    )\n",
      "    (transition2): TransmitBlock(\n",
      "      (norm): BatchNorm3d(112, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (act): LeakyReLU(negative_slope=0.1, inplace=True)\n",
      "      (conv): Conv3d(112, 56, kernel_size=(1, 1, 1), stride=(1, 1, 1))\n",
      "      (pool): AvgPool3d(kernel_size=2, stride=2, padding=0)\n",
      "    )\n",
      "    (denseblock3_layer1): ConvBlock(\n",
      "      (norm_1): BatchNorm3d(56, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (act_1): LeakyReLU(negative_slope=0.1, inplace=True)\n",
      "      (conv_1): Conv3d(56, 128, kernel_size=(1, 1, 1), stride=(1, 1, 1))\n",
      "      (norm_2): BatchNorm3d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (act_2): LeakyReLU(negative_slope=0.1, inplace=True)\n",
      "      (conv_2): Conv3d(128, 32, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))\n",
      "    )\n",
      "    (denseblock3_layer2): ConvBlock(\n",
      "      (norm_1): BatchNorm3d(88, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (act_1): LeakyReLU(negative_slope=0.1, inplace=True)\n",
      "      (conv_1): Conv3d(88, 128, kernel_size=(1, 1, 1), stride=(1, 1, 1))\n",
      "      (norm_2): BatchNorm3d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (act_2): LeakyReLU(negative_slope=0.1, inplace=True)\n",
      "      (conv_2): Conv3d(128, 32, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))\n",
      "    )\n",
      "    (transition3): TransmitBlock(\n",
      "      (norm): BatchNorm3d(120, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (act): LeakyReLU(negative_slope=0.1, inplace=True)\n",
      "    )\n",
      "  )\n",
      "  (classifier): Linear(in_features=120, out_features=2, bias=True)\n",
      ")\n",
      "Random initialized.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/vivi/anaconda3/envs/3dunet/lib/python3.7/site-packages/ipykernel_launcher.py:30: FutureWarning: get_value is deprecated and will be removed in a future release. Please use .at[] or .iat[] accessors instead\n"
     ]
    }
   ],
   "source": [
    "# '''[begin] load the train and validation dataset:'''\n",
    "# import numpy as np\n",
    "# # use np.load to import data, devide dataset into 2 parts: train_data & validation_data:\n",
    "# # credit: cheez & Matthew Kerian\n",
    "# # link: https://stackoverflow.com/questions/55890813/how-to-fix-object-arrays-cannot-be-loaded-when-allow-pickle-false-for-imdb-loa/56243777\n",
    "# '''use older to successfully load the data:'''\n",
    "# # save np.load\n",
    "# np_load_old = np.load\n",
    "\n",
    "# # modify the default parameters of np.load\n",
    "# np.load = lambda *a,**k: np_load_old(*a, allow_pickle=True, **k)\n",
    "\n",
    "# data_ineed = np.load('muchdata.npy')\n",
    "\n",
    "# # restore np.load for future normal usage\n",
    "# np.load = np_load_old\n",
    "\n",
    "# # If you are working with the basic sample data, use maybe 2 instead of 100 here... you don't have enough data to really do this\n",
    "# train_data = data_ineed[:-60]\n",
    "# validation_data = data_ineed[-60:]\n",
    "# '''[end] load the train and validation dataset:'''\n",
    "\n",
    "model = densenet3d(with_segment=False, use_memonger=True).cuda()\n",
    "# model = nn.DataParallel(model, device_ids=[0, 1])\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "# optimizer = optim.SGD(model.parameters(),lr=0.001, momentum=0.9)\n",
    "\n",
    "criterion = criterion.cuda()\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "data_train = ClfDataset(train=True)\n",
    "train_data_loader = DataLoader(dataset=data_train, batch_size=32, shuffle=False)\n",
    "dev_data_loader = DataLoader(dataset=data_train, batch_size=32, shuffle=False)\n",
    "data_test = ClfDataset(train=False)\n",
    "test_data_loader = DataLoader(dataset=data_test, batch_size=32, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train & Valiation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 15/15 [00:07<00:00,  1.95it/s]\n",
      "  0%|          | 0/15 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1] loss: 0.03142\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 15/15 [00:07<00:00,  1.98it/s]\n",
      "  0%|          | 0/15 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2] loss: 0.02971\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 15/15 [00:07<00:00,  1.94it/s]\n",
      "  0%|          | 0/15 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[3] loss: 0.02862\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 15/15 [00:07<00:00,  1.96it/s]\n",
      "  0%|          | 0/15 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[4] loss: 0.02793\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 15/15 [00:07<00:00,  1.96it/s]\n",
      "  0%|          | 0/15 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[5] loss: 0.02731\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 15/15 [00:07<00:00,  1.95it/s]\n",
      "  0%|          | 0/15 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[6] loss: 0.02711\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 15/15 [00:07<00:00,  1.94it/s]\n",
      "  0%|          | 0/15 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[7] loss: 0.02698\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 15/15 [00:07<00:00,  1.94it/s]\n",
      "  0%|          | 0/15 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[8] loss: 0.02557\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 15/15 [00:07<00:00,  1.93it/s]\n",
      "  0%|          | 0/15 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[9] loss: 0.02529\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 15/15 [00:07<00:00,  1.93it/s]\n",
      "  0%|          | 0/15 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[10] loss: 0.02477\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 15/15 [00:07<00:00,  1.92it/s]\n",
      "  0%|          | 0/15 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[11] loss: 0.02395\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 15/15 [00:07<00:00,  1.91it/s]\n",
      "  0%|          | 0/15 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[12] loss: 0.02351\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 15/15 [00:07<00:00,  1.89it/s]\n",
      "  0%|          | 0/15 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[13] loss: 0.02213\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 15/15 [00:07<00:00,  1.89it/s]\n",
      "  0%|          | 0/15 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[14] loss: 0.02177\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 15/15 [00:07<00:00,  1.89it/s]\n",
      "  0%|          | 0/15 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[15] loss: 0.01991\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 15/15 [00:07<00:00,  1.88it/s]\n",
      "  0%|          | 0/15 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[16] loss: 0.01819\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 15/15 [00:08<00:00,  1.87it/s]\n",
      "  0%|          | 0/15 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[17] loss: 0.01827\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 15/15 [00:07<00:00,  1.89it/s]\n",
      "  0%|          | 0/15 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[18] loss: 0.01912\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 15/15 [00:08<00:00,  1.87it/s]\n",
      "  0%|          | 0/15 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[19] loss: 0.01800\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 15/15 [00:07<00:00,  1.88it/s]\n",
      "  0%|          | 0/15 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[20] loss: 0.01609\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 15/15 [00:08<00:00,  1.86it/s]\n",
      "  0%|          | 0/15 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[21] loss: 0.01558\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 15/15 [00:08<00:00,  1.87it/s]\n",
      "  0%|          | 0/15 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[22] loss: 0.01468\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 15/15 [00:08<00:00,  1.87it/s]\n",
      "  0%|          | 0/15 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[23] loss: 0.01447\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 15/15 [00:08<00:00,  1.85it/s]\n",
      "  0%|          | 0/15 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[24] loss: 0.01426\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 15/15 [00:08<00:00,  1.86it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[25] loss: 0.01305\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "date = 24 # day\n",
    "number = 7 # No.\n",
    "file_name = 'train_{}_{}'.format(date,number)\n",
    "# writer = SummaryWriter('./runs/{}'.format(file_name))\n",
    "logger = tb_logger.Logger(logdir = './runs/train_{}_{}'.format(date,number), flush_secs=2)\n",
    "# val_logger = tb_logger.Logger(logdir = './runs/train_{}_{}/test'.format(date,number), flush_secs=2)\n",
    "\n",
    "for epoch in range(25):  # loop over the dataset multiple times\n",
    "    \n",
    "    # for train:\n",
    "    running_loss = 0.0\n",
    "    # for valiation:\n",
    "    dev_loss = 0\n",
    "    predict_value = torch.zeros(1) # valiation predict:\n",
    "    true_value = torch.zeros(1) # true lable of valiation part:\n",
    "    \n",
    "    for index, (inputs, labels) in enumerate(tqdm(train_data_loader)):\n",
    "        \n",
    "        if index<=100:# train part: \n",
    "            \n",
    "            model.train()\n",
    "            \n",
    "            # zero the parameter gradients\n",
    "            optimizer.zero_grad()\n",
    "        \n",
    "            inputs, labels = inputs.cuda(), labels.cuda()\n",
    "        \n",
    "            # forward + backward + optimize\n",
    "            inputs = inputs.unsqueeze(dim=1).float()\n",
    "        \n",
    "            inputs = F.interpolate(inputs, size=[32,32,32],mode='trilinear',align_corners=False)\n",
    "        \n",
    "            outputs = model(inputs)\n",
    "        \n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        \n",
    "            # print statistics\n",
    "            running_loss += loss.item()\n",
    "            \n",
    "        else: # valiation part:\n",
    "            \n",
    "            model.eval()\n",
    "            \n",
    "            inputs, labels = inputs.cuda(), labels.cuda()\n",
    "        \n",
    "            # forward + backward + optimize\n",
    "            inputs = inputs.unsqueeze(dim=1).float()\n",
    "        \n",
    "            inputs = F.interpolate(inputs, size=[32,32,32],mode='trilinear',align_corners=False)\n",
    "        \n",
    "            outputs = model(inputs)\n",
    "        \n",
    "            test_value = F.softmax(outputs)\n",
    "            test_value_1 = test_value[:,1]\n",
    "            test_value_1 = test_value_1.cpu()\n",
    "            predict_value = torch.cat((predict_value,test_value_1))\n",
    "    \n",
    "            test_value_2 = labels\n",
    "            test_value_2 = test_value_2.cpu().float()\n",
    "            true_value = torch.cat((true_value, test_value_2))\n",
    "            \n",
    "    print('[%d] loss: %.5f' %(epoch + 1, running_loss / 352))\n",
    "#     writer.add_scalar('Loss', running_loss, epoch)\n",
    "    logger.log_value('loss',running_loss,epoch)\n",
    "    running_loss = 0.0\n",
    "        \n",
    "#     predict_value = predict_value.detach().numpy()\n",
    "#     true_value = true_value.numpy()\n",
    "#     auc_score = roc_auc_score(true_value, predict_value)\n",
    "#     print(auc_score)\n",
    "# #     writer.add_scalar('Auc', auc_score, epoch)\n",
    "#     logger.log_value('AUC',auc_score, epoch)\n",
    "    \n",
    "# writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH = './my_{}_{}.pth'.format(date,number)\n",
    "torch.save(model.state_dict(), PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ./my_23_1.pth 100epochs\n",
    "# my_24_1.pth 300epochs train&dev Adam 0.62(200) batch_size = 32\n",
    "# my_24_2.pth 200epochs train&dev SGD 0.656(200) batch_size = 32\n",
    "# my_24_3.pth 200epochs train&dev SGD 0.633(200) batch_size = 16\n",
    "# my_24_4.pth 200epochs train&dev SGD 0.625(200) batch_size = 32 crop 80-> 32\n",
    "# my_24_5.pth 200epochs train&dev Adam 0.600(200) batch_size = 32 crop 80-> 32 0.689(20)\n",
    "# my_24_6.pth  25epochs train&dev Adam           batch_size = 32 crop 80-> 32  0.689(25)\n",
    "# my_24_7.pth  25epochs train     Adam           batch_size = 32 crop 80-> 32             test:  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/4 [00:00<?, ?it/s]/home/vivi/anaconda3/envs/3dunet/lib/python3.7/site-packages/ipykernel_launcher.py:20: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      " 50%|█████     | 2/4 [00:00<00:00,  4.15it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.0163, 0.0438, 0.0047, 0.0895, 0.2656, 0.7979, 0.0160, 0.0961, 0.9794,\n",
      "        0.8657, 0.8988, 0.8564, 0.2909, 0.9786, 0.0414, 0.4130, 0.7972, 0.0016,\n",
      "        0.8145, 0.8647, 0.9426, 0.7954, 0.7167, 0.9997, 0.0303, 0.3989, 0.8806,\n",
      "        0.0206, 0.0622, 0.3059, 0.0874, 0.2412], grad_fn=<CopyBackwards>)\n",
      "tensor([6.4061e-01, 2.8064e-02, 1.2445e-02, 2.1896e-01, 1.5693e-01, 1.9309e-01,\n",
      "        1.6293e-01, 2.0663e-01, 9.8380e-01, 4.0963e-02, 9.1121e-01, 9.7009e-01,\n",
      "        7.9719e-01, 2.6561e-01, 2.2475e-02, 2.5949e-01, 2.4494e-01, 2.5203e-02,\n",
      "        1.9382e-04, 9.9655e-01, 4.7924e-01, 7.1119e-01, 7.9070e-01, 1.8731e-02,\n",
      "        8.5209e-01, 9.9854e-01, 1.4750e-02, 9.8679e-01, 8.7956e-02, 1.9940e-01,\n",
      "        2.8204e-01, 9.1411e-01], grad_fn=<CopyBackwards>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4/4 [00:00<00:00,  5.99it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.4917, 0.0045, 0.3156, 0.4285, 0.1786, 0.1338, 0.5182, 0.0742, 0.0229,\n",
      "        0.1707, 0.0124, 0.0050, 0.5133, 0.1499, 0.8591, 0.0083, 0.9873, 0.9522,\n",
      "        0.0745, 0.6570, 0.0280, 0.3440, 0.9953, 0.3791, 0.4480, 0.1002, 0.1234,\n",
      "        0.9992, 0.9388, 0.0335, 0.2724, 0.0070], grad_fn=<CopyBackwards>)\n",
      "tensor([4.0067e-01, 1.9767e-01, 6.6798e-01, 9.5948e-01, 6.5158e-04, 2.6652e-02,\n",
      "        1.1415e-01, 7.4760e-03, 5.8621e-01, 5.8227e-01, 5.7421e-01, 7.8152e-01,\n",
      "        7.7834e-01, 1.1900e-01, 1.1193e-01, 2.8672e-02, 6.0612e-02, 9.8527e-01,\n",
      "        7.4148e-01, 4.1322e-01, 9.7131e-01], grad_fn=<CopyBackwards>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# PATH = './my_{}_{}.pth'.format(date,number)\n",
    "# model = densenet3d(with_segment=False, use_memonger=True).cuda()\n",
    "# model.load_state_dict(torch.load(PATH))\n",
    "\n",
    "model.eval()\n",
    "# dev_loss = 0\n",
    "predict_value = torch.zeros(1)\n",
    "# true_value = torch.zeros(1)\n",
    "\n",
    "for _, inputs in enumerate(tqdm(test_data_loader)):\n",
    "    \n",
    "    inputs = inputs.cuda()\n",
    "        # forward + backward + optimize\n",
    "    inputs = inputs.unsqueeze(dim=1).float()\n",
    "        \n",
    "    inputs = F.interpolate(inputs, size=[32,32,32],mode='trilinear',align_corners=False)\n",
    "        \n",
    "    outputs = model(inputs)\n",
    "    \n",
    "    test_value = F.softmax(outputs)\n",
    "    test_value_1 = test_value[:,1]\n",
    "    test_value_1 = test_value_1.cpu()\n",
    "    print(test_value_1)\n",
    "    \n",
    "    predict_value = torch.cat((predict_value,test_value_1))\n",
    "    \n",
    "#     test_value_2 = labels\n",
    "#     test_value_2 = test_value_2.cpu().float()\n",
    "#     true_value = torch.cat((true_value, test_value_2))\n",
    "\n",
    "\n",
    "test_predict_value = predict_value.detach().numpy()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.        , 0.7681988 , 0.21929917, 0.25306442, 0.13273145,\n",
       "       0.4268722 , 0.74537003, 0.3375637 , 0.69582593, 0.09741792,\n",
       "       0.43878582, 0.16675016, 0.61610097, 0.44171885, 0.4440905 ,\n",
       "       0.13823506, 0.35815305, 0.5310992 , 0.15788056, 0.42568162,\n",
       "       0.6047288 , 0.35053736, 0.70405084, 0.5185141 , 0.8323816 ,\n",
       "       0.03381161, 0.3956179 , 0.37155223, 0.03907281, 0.53791827,\n",
       "       0.3630199 , 0.47596002, 0.26511502, 0.47440252, 0.30700487,\n",
       "       0.03689992, 0.42741382, 0.31713384, 0.27939385, 0.640623  ,\n",
       "       0.4235127 , 0.4246907 , 0.62693954, 0.52634645, 0.49056748,\n",
       "       0.7664389 , 0.4124694 , 0.196875  , 0.37485656, 0.08014497,\n",
       "       0.12605228, 0.16157927, 0.87304276, 0.7250747 , 0.10267787,\n",
       "       0.46728963, 0.3557893 , 0.6340106 , 0.76406246, 0.5415598 ,\n",
       "       0.5741628 , 0.26083955, 0.39410833, 0.79902196, 0.94410497,\n",
       "       0.39074177, 0.506341  , 0.90756494, 0.6660692 , 0.14984173,\n",
       "       0.42179722, 0.49830067, 0.6371415 , 0.39978033, 0.6324098 ,\n",
       "       0.16804555, 0.38610777, 0.35723191, 0.1469556 , 0.8409432 ,\n",
       "       0.03510322, 0.8292097 , 0.73407894, 0.75371486, 0.39081228,\n",
       "       0.64242154, 0.41837496, 0.8399079 , 0.7960951 , 0.17793587,\n",
       "       0.38404688, 0.48093712, 0.73250556, 0.20264077, 0.12764981,\n",
       "       0.21556647, 0.23978947, 0.7338282 , 0.47091696, 0.6775122 ,\n",
       "       0.6559636 , 0.10885134, 0.26072142, 0.4016154 , 0.07439169,\n",
       "       0.87145865, 0.66152424, 0.40394047, 0.7587935 , 0.21625802,\n",
       "       0.2758433 , 0.28441626, 0.0672313 , 0.1368018 , 0.14164089,\n",
       "       0.6485921 , 0.05450958, 0.6346656 ], dtype=float32)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_predict_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.savetxt('new_{}_{}.csv'.format(date,number), test_predict_value, delimiter = ',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for epoch in range(4):  # loop over the dataset multiple times\n",
    "#     running_loss = 0.0\n",
    "#     for inputs, labels in train_data_loader:\n",
    "#         # get the inputs; data is a list of [inputs, labels]\n",
    "#         # inputs: tensor(100*(100*100)), label\n",
    "#         inputs_numpy, labels = data\n",
    "#         inputs = torch.from_numpy(inputs_numpy)\n",
    "#         inputs = inputs.unsqueeze(0)\n",
    "#         inputs = inputs.unsqueeze(0)\n",
    "#         inputs = inputs.cuda()\n",
    "# #         inputs = inputs.float()\n",
    "#         labels = torch.from_numpy(np.asarray(labels)).float()\n",
    "#         labels = labels.long() #credit: ptrblck; link: https://discuss.pytorch.org/t/runtimeerror-expected-object-of-scalar-type-long-but-got-scalar-type-float-when-using-crossentropyloss/30542/4\n",
    "#         labels = labels.unsqueeze(0)\n",
    "\n",
    "#         labels = labels.cuda()\n",
    "#         # zero the parameter gradients\n",
    "#         optimizer.zero_grad()\n",
    "\n",
    "#         # forward + backward + optimize\n",
    "#         outputs = model(inputs)\n",
    "        \n",
    "#         loss = criterion(outputs, labels)\n",
    "#         loss.backward()\n",
    "#         optimizer.step()\n",
    "\n",
    "#         # print statistics\n",
    "#         running_loss += loss.item()\n",
    "#         if i % 20 == 19:    # print every 2000 mini-batches\n",
    "#             print('[%d, %5d] loss: %.3f' %\n",
    "#                   (epoch + 1, i + 1, running_loss / 20))\n",
    "#             running_loss = 0.0\n",
    "\n",
    "# print('Finished Training')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Only train:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# for epoch in range(100):  # loop over the dataset multiple times\n",
    "    \n",
    "#     model.train()\n",
    "#     running_loss = 0.0\n",
    "#     for index, (inputs, labels) in enumerate(tqdm(train_data_loader)):\n",
    "#         # zero the parameter gradients\n",
    "#         optimizer.zero_grad()\n",
    "        \n",
    "#         inputs, labels = inputs.cuda(), labels.cuda()\n",
    "        \n",
    "#         # forward + backward + optimize\n",
    "#         inputs = inputs.unsqueeze(dim=1).float()\n",
    "        \n",
    "#         inputs = F.interpolate(inputs, size=[32,32,32],mode='trilinear',align_corners=False)\n",
    "        \n",
    "#         outputs = model(inputs)\n",
    "        \n",
    "#         loss = criterion(outputs, labels)\n",
    "#         loss.backward()\n",
    "#         optimizer.step()\n",
    "        \n",
    "#         # print statistics\n",
    "#         running_loss += loss.item()\n",
    "        \n",
    "#         if index==11:\n",
    "#             break\n",
    "        \n",
    "\n",
    "#     print('[%d] loss: %.5f' %\n",
    "#         (epoch + 1, running_loss / 352))\n",
    "#     running_loss = 0.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Only Valiation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PATH = './my_23_1.pth'\n",
    "# model = densenet3d(with_segment=False, use_memonger=True).cuda()\n",
    "# model.load_state_dict(torch.load(PATH))\n",
    "\n",
    "# model.eval()\n",
    "# dev_loss = 0\n",
    "# predict_value = torch.zeros(1)\n",
    "# true_value = torch.zeros(1)\n",
    "\n",
    "# for index, (inputs, labels) in enumerate(tqdm(train_data_loader)):\n",
    "#     if index<=11:\n",
    "#         continue\n",
    "\n",
    "#     inputs, labels = inputs.cuda(), labels.cuda()\n",
    "        \n",
    "#         # forward + backward + optimize\n",
    "#     inputs = inputs.unsqueeze(dim=1).float()\n",
    "        \n",
    "#     inputs = F.interpolate(inputs, size=[32,32,32],mode='trilinear',align_corners=False)\n",
    "        \n",
    "#     outputs = model(inputs)\n",
    "    \n",
    "#     test_value = F.softmax(outputs)\n",
    "#     test_value_1 = test_value[:,1]\n",
    "#     test_value_1 = test_value_1.cpu()\n",
    "#     predict_value = torch.cat((predict_value,test_value_1))\n",
    "    \n",
    "#     test_value_2 = labels\n",
    "#     test_value_2 = test_value_2.cpu().float()\n",
    "#     true_value = torch.cat((true_value, test_value_2))\n",
    "# #     loss = criterion(outputs, labels)    \n",
    "# #     dev_loss+=loss.item()\n",
    "    \n",
    "# # dev_loss = dev_loss/64\n",
    "# # print(\"%.4f\" %dev_loss)\n",
    "\n",
    "# predict_value = predict_value.detach().numpy()\n",
    "# true_value = true_value.numpy()\n",
    "\n",
    "# from sklearn.metrics import roc_auc_score\n",
    "# roc_auc_score(true_value, predict_value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
