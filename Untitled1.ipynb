{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import __init__paths\n",
    "import os\n",
    "import argparse\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "import json\n",
    "from tensorboardX import SummaryWriter\n",
    "\n",
    "from utils.loss import dice_loss\n",
    "from utils.util import segment2n_segment\n",
    "from model.convshape import ConvShape\n",
    "from model.densenet2SAT import Features2SAT, PositionalEncoding\n",
    "from model.densenet3d import densenet3d\n",
    "from dataloader.dataset_quantified import ClfDataset\n",
    "\n",
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument('--nepoch', type=int, default=200, help='number of epochs to train for')\n",
    "parser.add_argument('--train_batch', type=int, default=32, help='batch that get backward')\n",
    "parser.add_argument('--batch_size', type=int, default=16, help='dataset and test batch size')\n",
    "parser.add_argument('--n_sat', type=int, default=1024, help='control the number of sat')\n",
    "parser.add_argument('--file_name', type=str, default='', help='file name')\n",
    "parser.add_argument('--write_log', type=bool, default=True, help='write log control')\n",
    "parser.add_argument('--load_model', type=bool, default=True, help='whether load model')\n",
    "parser.add_argument('--load_model_file', type=str, default='', help='dictionary of load model')\n",
    "opt = parser.parse_args()\n",
    "print(opt)\n",
    "\n",
    "divide_batch = opt.train_batch // opt.batch_size\n",
    "device_ids = [0, 1]\n",
    "\n",
    "\n",
    "def train(epoch, model, criterion, optimizer, writer, opt, data_loader):\n",
    "    model['densesharp'].train()\n",
    "    model['sat'].train()\n",
    "    total_cross_loss = 0\n",
    "    total_dice_loss = 0\n",
    "    eval_loss = 0\n",
    "    correct = 0\n",
    "    positive_target = np.zeros(len(data_loader.dataset))\n",
    "    positive_score = np.zeros(len(data_loader.dataset))\n",
    "    optimizer.zero_grad()\n",
    "    for batch_idx, (data, target, segment_target) in enumerate(tqdm(data_loader)):\n",
    "        if torch.cuda.is_available():\n",
    "            data, target, segment_target = data.cuda(), target.cuda(), segment_target.cuda()\n",
    "\n",
    "        # forward the models\n",
    "        output, features, segment_output = model['densesharp'](data)\n",
    "        n_segment = segment2n_segment(segment_target, opt.n_sat)\n",
    "        batch_features = model['feature2SAT'](features, n_segment)\n",
    "        batch_features = batch_features + model['p_encoder'](n_segment)\n",
    "        output_SAT = model['sat'](batch_features)\n",
    "\n",
    "        # get the loss\n",
    "        indiv_cross_loss = criterion(output_SAT, target)\n",
    "        indiv_dice_loss = dice_loss(segment_output, segment_target)\n",
    "        loss = indiv_cross_loss + 0.2 * indiv_dice_loss\n",
    "        loss.backward()\n",
    "        total_cross_loss += indiv_cross_loss.item()\n",
    "        total_dice_loss += indiv_dice_loss.item()\n",
    "        eval_loss += loss.item()\n",
    "\n",
    "        pred = output_SAT.data.max(1, keepdim=True)[1]\n",
    "        correct += pred.eq(target.data.view_as(pred)).cpu().sum()\n",
    "\n",
    "        possi = F.softmax(output_SAT, dim=1)\n",
    "        for i, t in enumerate(target):\n",
    "            pos = opt.batch_size * batch_idx + i\n",
    "            positive_target[pos] = target.data[i]\n",
    "            positive_score[pos] = possi.cpu().data[i][0]\n",
    "\n",
    "        if (batch_idx+1) % divide_batch == 0:\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "        if batch_idx % 10 == 0:\n",
    "            log_tmp = 'Train Epoch: {} [{}/{} ({:.0f}%)]\\tCrossEntropyLoss: {:.6f}\\tDiceLoss: {:.6f}\\tLoss: {:.6f}'.format(\n",
    "                epoch, batch_idx * len(data), len(data_loader.dataset),\n",
    "                100. * batch_idx / len(data_loader), indiv_cross_loss.item() / opt.batch_size,\n",
    "                indiv_dice_loss.item(), loss.item())\n",
    "            print(log_tmp)\n",
    "            if opt.write_log:\n",
    "                with open(\"./log/{}.txt\".format(opt.file_name), \"a\") as log:\n",
    "                    log.write('{}\\n'.format(log_tmp))\n",
    "\n",
    "    eval_loss /= len(data_loader.dataset)\n",
    "    total_cross_loss /= len(data_loader.dataset)\n",
    "    total_dice_loss = total_dice_loss / (len(data_loader.dataset) / opt.batch_size)\n",
    "    log_tmp = 'Eval Epoch:{} CrossEntropyLoss:{:.6f} DiceLoss:{:.6f} Average loss: {:.6f}, Accuracy: {}/{} ({:.6f}%)'.format(\n",
    "        epoch, total_cross_loss, total_dice_loss, eval_loss, correct, len(data_loader.dataset),\n",
    "        100. * float(correct) / len(data_loader.dataset))\n",
    "    print(log_tmp)\n",
    "\n",
    "    # draw the ROC curve\n",
    "    fpr, tpr, thresholds = roc_curve(positive_target, positive_score, pos_label=0)\n",
    "    roc_auc = auc(fpr, tpr)\n",
    "    print('Train_AUC = %.8f' % roc_auc)\n",
    "\n",
    "    if opt.write_log:\n",
    "        with open(\"./data/{}/epoch{}_train_fpr.json\".format(opt.file_name, epoch), \"w\") as f:\n",
    "            json.dump(fpr.tolist(), f)\n",
    "        with open(\"./data/{}/epoch{}_train_tpr.json\".format(opt.file_name, epoch), \"w\") as f:\n",
    "            json.dump(tpr.tolist(), f)\n",
    "        with open(\"./log/{}.txt\".format(opt.file_name), \"a\") as log:\n",
    "            log.write('{}\\n'.format(log_tmp))\n",
    "            log.write('Train_AUC = %.8f\\n' % roc_auc)\n",
    "        writer.add_scalar('Train_AUC', roc_auc, epoch)\n",
    "        writer.add_scalar('Train_CrossEntropyLoss', total_cross_loss, epoch)\n",
    "        writer.add_scalar('Train_DiceLoss', total_dice_loss, epoch)\n",
    "        writer.add_scalar('Eval_loss', eval_loss, epoch)\n",
    "        writer.add_scalar('Eval_accuracy', 100. * float(correct) / len(data_loader.dataset), epoch)\n",
    "        torch.save(model['densesharp'].state_dict(), './model_saved/{}/densesharp_{}_epoch_{}_dict.pkl'\n",
    "                   .format(opt.file_name, opt.file_name, epoch))\n",
    "        torch.save(model['sat'].state_dict(), './model_saved/{}/sat_{}_epoch_{}_dict.pkl'\n",
    "                   .format(opt.file_name, opt.file_name, epoch))\n",
    "\n",
    "\n",
    "def test(epoch, model, criterion, writer, opt, data_loader, dichotomy=False):\n",
    "    model['densesharp'].eval()\n",
    "    model['sat'].eval()\n",
    "    total_cross_loss = 0\n",
    "    total_dice_loss = 0\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    positive_target = np.zeros(len(data_loader.dataset))\n",
    "    positive_score = np.zeros(len(data_loader.dataset))\n",
    "    for index, (data, target, segment_target) in enumerate(tqdm(data_loader)):\n",
    "        if torch.cuda.is_available():\n",
    "            data, target, segment_target = data.cuda(), target.cuda(), segment_target.cuda()\n",
    "\n",
    "        # forward the models\n",
    "        output, features, segment_output = model['densesharp'](data)\n",
    "        n_segment = segment2n_segment(segment_target, opt.n_sat)\n",
    "        batch_features = model['feature2SAT'](features, n_segment)\n",
    "        batch_features = batch_features + model['p_encoder'](n_segment)\n",
    "        output_SAT = model['sat'](batch_features)\n",
    "\n",
    "        # sum up batch loss\n",
    "        indiv_cross_loss = criterion(output_SAT, target)\n",
    "        indiv_dice_loss = dice_loss(segment_output, segment_target)\n",
    "        loss = indiv_cross_loss + 0.2 * indiv_dice_loss\n",
    "        total_cross_loss += indiv_cross_loss.item()\n",
    "        total_dice_loss += indiv_dice_loss.item()\n",
    "        test_loss += loss.item()\n",
    "\n",
    "        # get the index of the max log-probability\n",
    "        pred = output_SAT.data.max(1, keepdim=True)[1]\n",
    "        correct += pred.eq(target.data.view_as(pred)).cpu().sum()\n",
    "\n",
    "        if dichotomy:\n",
    "            possi = F.softmax(output_SAT, dim=1)\n",
    "            for i, t in enumerate(target):\n",
    "                pos = opt.batch_size * index + i\n",
    "                positive_target[pos] = target.data[i]\n",
    "                positive_score[pos] = possi.cpu().data[i][0]\n",
    "\n",
    "    total_cross_loss /= len(data_loader.dataset)\n",
    "    total_dice_loss /= (len(data_loader.dataset) / opt.batch_size)\n",
    "    test_loss /= len(data_loader.dataset)\n",
    "    log_tmp = 'Test epoch:{} CrossEntropyLoss:{:.6f} DiceLoss:{:.6f} Average loss:{:.6f}, Accuracy: {}/{} ({:.6f}%)'.format(\n",
    "        epoch, total_cross_loss, total_dice_loss, test_loss, correct, len(data_loader.dataset),\n",
    "        100. * float(correct) / len(data_loader.dataset))\n",
    "    print(log_tmp)\n",
    "\n",
    "    # draw the ROC curve\n",
    "    fpr, tpr, thresholds = roc_curve(positive_target, positive_score, pos_label=0)\n",
    "    roc_auc = auc(fpr, tpr)\n",
    "    print('AUC = %.8f' % roc_auc)\n",
    "\n",
    "    if opt.write_log:\n",
    "        with open(\"./log/{}.txt\".format(opt.file_name), \"a\") as log:\n",
    "            log.write('{}\\n'.format(log_tmp))\n",
    "        writer.add_scalar('Test_AUC', roc_auc, epoch)\n",
    "        writer.add_scalar('Test_loss', test_loss, epoch)\n",
    "        writer.add_scalar('Test_CrossEntropyLoss', total_cross_loss, epoch)\n",
    "        writer.add_scalar('Test_DiceLoss', total_dice_loss, epoch)\n",
    "        writer.add_scalar('Test_accuracy', 100. * float(correct) / len(data_loader.dataset), epoch)\n",
    "\n",
    "    if dichotomy and opt.write_log:\n",
    "\n",
    "        with open(\"./log/{}.txt\".format(opt.file_name), \"a\") as log:\n",
    "            log.write('Test_AUC = %.8f\\n' % roc_auc)\n",
    "        with open(\"./data/{}/epoch{}_test_fpr.json\".format(opt.file_name, epoch), \"w\") as f:\n",
    "            json.dump(fpr.tolist(), f)\n",
    "        with open(\"./data/{}/epoch{}_test_tpr.json\".format(opt.file_name, epoch), \"w\") as f:\n",
    "            json.dump(tpr.tolist(), f)\n",
    "\n",
    "\n",
    "def main():\n",
    "    for eval_set in range(1, 6):\n",
    "        opt.file_name = 'both_train_no_segment_SAT_3_8_4_crossval_{}_batch_{}'.format(eval_set, opt.train_batch)\n",
    "\n",
    "        writer = SummaryWriter('./runs/{}_epoch_{}'.format(opt.file_name, opt.nepoch))\n",
    "        if opt.write_log:\n",
    "            if not os.path.exists('./data/{}'.format(opt.file_name)):\n",
    "                os.makedirs('./data/{}'.format(opt.file_name))\n",
    "\n",
    "            if not os.path.exists('./model_saved/{}'.format(opt.file_name)):\n",
    "                os.makedirs('./model_saved/{}'.format(opt.file_name))\n",
    "\n",
    "        train_subset = [1, 2, 3, 4, 5]\n",
    "        train_subset.remove(eval_set)\n",
    "        train_dataset = ClfDataset(train=True, crop_size=32, move=5, subset=train_subset, output_segment=True)\n",
    "        train_data_loader = DataLoader(train_dataset, batch_size=opt.batch_size, shuffle=False, sampler=None)\n",
    "\n",
    "        test_dataset = ClfDataset(train=False, crop_size=32,  subset=[eval_set], output_segment=True)\n",
    "        test_data_loader = DataLoader(test_dataset, batch_size=opt.batch_size, shuffle=False, sampler=None)\n",
    "\n",
    "        # define models\n",
    "        pkl_step_list = [159, 124, 166, 132, 28]\n",
    "        pkl_name = 'denseSharp_4_4_4_dichotomy_layer_crossval_{}_dataset_60_epoch_{}_dict.pkl'.format(\n",
    "            eval_set, pkl_step_list[eval_set - 1])\n",
    "        opt.load_model_file = './model_saved/denseSharp_4_4_4_dichotomy_layer_crossval_{}_dataset_60/{}'.format(\n",
    "            eval_set, pkl_name)\n",
    "\n",
    "        # define models\n",
    "        criterion = nn.CrossEntropyLoss()\n",
    "        densesharp = densenet3d(with_segment=True)\n",
    "        feature2SAT = Features2SAT(batch_size=opt.batch_size, n_channel=120, n_sat=opt.n_sat, size=32)\n",
    "        positional_encoding = PositionalEncoding(n_channel=120, batch_size=opt.batch_size, n_sat=opt.n_sat)\n",
    "        sat = ConvShape(input_size=120, output_size=2, hidden_size=256, L=3, agg='avg')\n",
    "        if torch.cuda.is_available():\n",
    "            densesharp, feature2SAT, sat = densesharp.cuda(), feature2SAT.cuda(), sat.cuda()\n",
    "            densesharp = nn.DataParallel(densesharp, device_ids=device_ids)\n",
    "            sat = nn.DataParallel(sat, device_ids=device_ids)\n",
    "            criterion = criterion.cuda()\n",
    "\n",
    "        print('----- Start loading DenseSharp! -----')\n",
    "        if opt.load_model:\n",
    "            densesharp.load_state_dict(torch.load(opt.load_model_file))\n",
    "            print('----- Load finished! -----')\n",
    "        else:\n",
    "            print('----- Load failed! -----')\n",
    "\n",
    "        param = list(densesharp.parameters()) + list(sat.parameters())\n",
    "        optimizer = optim.Adam(param)\n",
    "        models = {'densesharp': densesharp, 'feature2SAT': feature2SAT, 'p_encoder': positional_encoding, 'sat': sat}\n",
    "        for epoch in range(opt.nepoch):\n",
    "            train(epoch, models, criterion, optimizer, writer, opt, train_data_loader)\n",
    "            test(epoch, models, criterion, writer, opt,  test_data_loader, dichotomy=True)\n",
    "        writer.close()\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
